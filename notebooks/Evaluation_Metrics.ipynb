{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39ec743f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/ec2-user/SageMaker/foodi-ml/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13f7145f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from addict import Dict\n",
    "\n",
    "import params\n",
    "from retrieval.train import train\n",
    "from retrieval.utils import helper\n",
    "from retrieval.model import loss\n",
    "from retrieval.model.model import Retrieval\n",
    "from retrieval.data.loaders import get_loaders\n",
    "from retrieval.utils.logger import create_logger\n",
    "from retrieval.utils.helper import load_model\n",
    "from retrieval.utils.file_utils import load_yaml_opts, parse_loader_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6399d9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation imports\n",
    "import retrieval.train.evaluation as evaluation \n",
    "from retrieval.model.similarity.similarity import Normalization\n",
    "from retrieval.model.similarity.measure import l2norm, cosine_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56c19d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizers(train_loader):\n",
    "    tokenizers = train_loader.dataset.tokenizer\n",
    "    if type(tokenizers) != list:\n",
    "        tokenizers = [tokenizers]\n",
    "    return tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1237329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_path(opt):\n",
    "    if 'DATA_PATH' not in os.environ:\n",
    "        if not opt.dataset.data_path:\n",
    "            raise Exception('''\n",
    "                DATA_PATH not specified.\n",
    "                Please, run \"$ export DATA_PATH=/path/to/dataset\"\n",
    "                or add path to yaml file\n",
    "            ''')\n",
    "        return opt.dataset.data_path\n",
    "    else:\n",
    "        return os.environ['DATA_PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a45987df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-18 17:44:15,457 - [INFO    ] - Loaded vocab containing 2487 tokens\n",
      "2021-08-18 17:44:15,457 - [INFO    ] - Loaded from .vocab_cache/foodiml_vocab.json.\n",
      "2021-08-18 17:44:15,458 - [INFO    ] - Created tokenizer with init 2487 tokens.\n",
      "2021-08-18 17:44:15,499 - [INFO    ] - [FoodiML] Loaded 5608 images and 5608 annotations.\n",
      "2021-08-18 17:44:15,503 - [INFO    ] - Loaded vocab containing 2487 tokens\n",
      "2021-08-18 17:44:15,503 - [INFO    ] - Loaded from .vocab_cache/foodiml_vocab.json.\n",
      "2021-08-18 17:44:15,503 - [INFO    ] - Created tokenizer with init 2487 tokens.\n",
      "2021-08-18 17:44:15,542 - [INFO    ] - [FoodiML] Loaded 2403 images and 2403 annotations.\n",
      "2021-08-18 17:44:15,544 - [INFO    ] - Adapt loaders: 0\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"DATA_PATH\"] = \"/home/ec2-user/SageMaker/data/\"\n",
    "\n",
    "options = \"options/adapt/foodi-ml/i2t.yaml\"\n",
    "\n",
    "args = {\"options\": options}\n",
    "args = Dict(args)\n",
    "opt = load_yaml_opts(args.options)\n",
    "\n",
    "logger = create_logger(level='info' if opt.engine.debug else 'info')\n",
    "\n",
    "# Get path of the data\n",
    "data_path = get_data_path(opt)\n",
    "\n",
    "# Get loaders\n",
    "train_loader, val_loaders, adapt_loaders = get_loaders(data_path, args.local_rank, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6d2ffae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers = get_tokenizers(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82bd7318",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-18 17:44:17,082 - [INFO    ] - Image encoder created: ('full_image',)\n",
      "2021-08-18 17:44:17,373 - [INFO    ] - Text encoder created: gru_glove\n",
      "2021-08-18 17:44:17,443 - [INFO    ] - Created similarity: AdaptiveEmbeddingI2T(\n",
      "  (norm): Normalization(\n",
      "    (norm): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  )\n",
      "  (adapt_txt): ADAPT(\n",
      "    (fc_gamma): Sequential(\n",
      "      (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    )\n",
      "    (fc_beta): Sequential(\n",
      "      (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (fovea): Fovea(smooth=10,train_smooth: False)\n",
      ")\n",
      "2021-08-18 17:44:20,406 - [INFO    ] - Setting devices: img: cuda,txt: cuda, loss: cuda\n",
      "2021-08-18 17:44:20,407 - [INFO    ] - Using similarity: ('adapt_i2t',)\n"
     ]
    }
   ],
   "source": [
    "model = Retrieval(**opt.model, tokenizers=tokenizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71ff49dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_criterion(opt, model):\n",
    "    if 'name' in opt.criterion:\n",
    "        logger.info(opt.criterion)\n",
    "        multimodal_criterion = loss.get_loss(**opt.criterion)\n",
    "        multilanguage_criterion = loss.get_loss(**opt.criterion)\n",
    "    else:\n",
    "        multimodal_criterion = loss.ContrastiveLoss(**opt.criterion)\n",
    "        multilanguage_criterion = loss.ContrastiveLoss(**opt.ml_criterion)\n",
    "    set_model_criterion(opt, model, multilanguage_criterion, multimodal_criterion)\n",
    "    # return multimodal_criterion, multilanguage_criterion\n",
    "\n",
    "\n",
    "def set_model_criterion(opt, model, multilanguage_criterion, multimodal_criterion):\n",
    "    model.mm_criterion = multimodal_criterion\n",
    "    model.ml_criterion = None\n",
    "    if len(opt.dataset.adapt.data) > 0:\n",
    "        model.ml_criterion = multilanguage_criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e275c344",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_fn = (lambda x: x) if not model.master else tqdm.write\n",
    "set_criterion(opt, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a36e3199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer\n",
    "trainer = train.Trainer(\n",
    "    model=model,\n",
    "    args=opt,\n",
    "    sysoutlog=print_fn,\n",
    "    path=opt.exp.outpath,\n",
    "    world_size=1 # TODO\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddf82e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model trained so far\n",
    "trainer.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "091fded6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrics, val_metrics = trainer.evaluate_loaders(valid_loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff8e6dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_metrics = {}\n",
    "final_sum = 0.\n",
    "nb_loaders = len(val_loaders)\n",
    "loader = val_loaders[0]\n",
    "loader_name = str(loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982020fb",
   "metadata": {},
   "source": [
    "##Â Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83b545b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    }
   ],
   "source": [
    "# Create embeddings\n",
    "img_emb, txt_emb, lens = evaluation.predict_loader(\n",
    "    trainer.model, \n",
    "    loader, \n",
    "    trainer.device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c31ae56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=trainer.model\n",
    "lengths=lens\n",
    "device=trainer.device\n",
    "shared_size=128\n",
    "return_sims=False\n",
    "latent_size = opt['model']['img_enc']['params']['img_dim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a468eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "_metrics_ = ('r1', 'r5', 'r10', 'medr', 'meanr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cddc4480",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_emb = torch.FloatTensor(img_emb)\n",
    "txt_emb = torch.FloatTensor(txt_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "260b2417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaptiveEmbeddingI2T: forward\n",
    "BB, LT, KK = img_emb.shape\n",
    "txt_emb = txt_emb.permute(0, 2, 1)\n",
    "if LT != latent_size:\n",
    "    print(\"Permutting image tensor\")\n",
    "    img_emb = img_emb.permute(0, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3153061e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Normalization layer and set to device\n",
    "norm = Normalization(latent_size, norm_method='batchnorm')\n",
    "norm = norm.to(device)\n",
    "txt_emb = txt_emb.to(device)\n",
    "txt_emb = norm(txt_emb)\n",
    "txt_emb_cpu = txt_emb.to('cpu')\n",
    "del norm\n",
    "del txt_emb\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ea27944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix to put results\n",
    "sims = torch.zeros(img_emb.shape[0], txt_emb_cpu.shape[0])\n",
    "\n",
    "# Global image representation\n",
    "img_emb = img_emb.mean(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0df99e9",
   "metadata": {},
   "source": [
    "###Â Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a21acfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = torch.zeros(sims.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1ce2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, img_tensor in enumerate(img_emb):\n",
    "    print(\"Iteration: \", i)\n",
    "    # Start 1,184 GB\n",
    "    img_vector_cpu = img_tensor.unsqueeze(0)\n",
    "\n",
    "    txt_emb = txt_emb_cpu.to(device)\n",
    "    img_vector = img_vector_cpu.to(device)\n",
    "    \n",
    "    # 2,639 GB\n",
    "    txt_output = trainer.model.similarity.similarity.adapt_txt(value=txt_emb, query=img_vector)\n",
    "    txt_output_fovea = trainer.model.similarity.similarity.fovea(txt_output)\n",
    "    txt_output_fovea_cpu = txt_output_fovea.to('cpu')\n",
    "    del txt_output\n",
    "    del txt_output_fovea\n",
    "    del txt_emb\n",
    "    del img_vector\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # 8,414 GB\n",
    "    txt_vector_cpu = txt_output_fovea_cpu.max(dim=-1)[0]\n",
    "    txt_vector_cpu = l2norm(txt_vector_cpu, dim=-1)\n",
    "    img_vector_cpu = l2norm(img_vector_cpu, dim=-1)\n",
    "    \n",
    "    # similarity\n",
    "    sim = cosine_sim(img_vector_cpu, txt_vector_cpu)\n",
    "    sim = sim.squeeze(-1)\n",
    "    sims[i,:] = sim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a50376",
   "metadata": {},
   "source": [
    "### Loop unfolded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8ba13ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start 1,184 GB\n",
    "i, img_tensor = (3, img_emb[3, :])\n",
    "img_vector_cpu = img_tensor.unsqueeze(0)\n",
    "\n",
    "txt_emb = txt_emb_cpu.to(device)\n",
    "img_vector = img_vector_cpu.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd0d8499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2,639 GB\n",
    "txt_output = trainer.model.similarity.similarity.adapt_txt(value=txt_emb, query=img_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ecb4466",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_output_fovea = trainer.model.similarity.similarity.fovea(txt_output)\n",
    "txt_output_fovea_cpu = txt_output_fovea.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2405668d",
   "metadata": {},
   "outputs": [],
   "source": [
    "del txt_output\n",
    "del txt_output_fovea\n",
    "del txt_emb\n",
    "del img_vector\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "66b36661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8,414 GB\n",
    "txt_vector_cpu = txt_output_fovea_cpu.max(dim=-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "06272a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_vector_cpu = l2norm(txt_vector_cpu, dim=-1)\n",
    "img_vector_cpu = l2norm(img_vector_cpu, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "230d4c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = cosine_sim(img_vector_cpu, txt_vector_cpu)\n",
    "sim = sim.squeeze(-1)\n",
    "sims[i,:] = sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "28dc387e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f1018d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sims = model.get_sim_matrix(\n",
    "#        embed_a=img_emb,\n",
    "#        embed_b=txt_emb,\n",
    "#        lens=lengths\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08d271e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0fe0174",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "564cd6ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3578141c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4621eb8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "17f5c110",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3cc386f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

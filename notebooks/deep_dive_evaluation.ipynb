{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19920479",
   "metadata": {},
   "source": [
    "# Deep Dive Evaluation during Training\n",
    "\n",
    "Execution \n",
    "\n",
    "```{bash}\n",
    "cd /home/ec2-user/SageMaker/foodi-ml\n",
    "source activate python3\n",
    "export DATA_PATH=/home/ec2-user/SageMaker/data/\n",
    "python run.py options/adapt/foodi-ml/i2t.yaml\n",
    "\n",
    "#nvidia-smi\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "872a4082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/ec2-user/SageMaker/foodi-ml/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "051b6e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from addict import Dict\n",
    "\n",
    "import params\n",
    "from retrieval.train import train\n",
    "from retrieval.utils import helper\n",
    "from retrieval.model import loss\n",
    "from retrieval.model.model import Retrieval\n",
    "from retrieval.data.loaders import get_loaders\n",
    "from retrieval.utils.logger import create_logger\n",
    "from retrieval.utils.helper import load_model\n",
    "from retrieval.utils.file_utils import load_yaml_opts, parse_loader_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7f4fb6",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47b566cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_path(opt):\n",
    "    if 'DATA_PATH' not in os.environ:\n",
    "        if not opt.dataset.data_path:\n",
    "            raise Exception('''\n",
    "                DATA_PATH not specified.\n",
    "                Please, run \"$ export DATA_PATH=/path/to/dataset\"\n",
    "                or add path to yaml file\n",
    "            ''')\n",
    "        return opt.dataset.data_path\n",
    "    else:\n",
    "        return os.environ['DATA_PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1956ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizers(train_loader):\n",
    "    tokenizers = train_loader.dataset.tokenizer\n",
    "    if type(tokenizers) != list:\n",
    "        tokenizers = [tokenizers]\n",
    "    return tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fa40d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_criterion(opt, model):\n",
    "    if 'name' in opt.criterion:\n",
    "        logger.info(opt.criterion)\n",
    "        multimodal_criterion = loss.get_loss(**opt.criterion)\n",
    "        multilanguage_criterion = loss.get_loss(**opt.criterion)\n",
    "    else:\n",
    "        multimodal_criterion = loss.ContrastiveLoss(**opt.criterion)\n",
    "        multilanguage_criterion = loss.ContrastiveLoss(**opt.ml_criterion)\n",
    "    set_model_criterion(opt, model, multilanguage_criterion, multimodal_criterion)\n",
    "    # return multimodal_criterion, multilanguage_criterion\n",
    "\n",
    "\n",
    "def set_model_criterion(opt, model, multilanguage_criterion, multimodal_criterion):\n",
    "    model.mm_criterion = multimodal_criterion\n",
    "    model.ml_criterion = None\n",
    "    if len(opt.dataset.adapt.data) > 0:\n",
    "        model.ml_criterion = multilanguage_criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b987bc9",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8728277",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-18 10:24:50,236 - [INFO    ] - Loaded vocab containing 2487 tokens\n",
      "2021-08-18 10:24:50,237 - [INFO    ] - Loaded from .vocab_cache/foodiml_vocab.json.\n",
      "2021-08-18 10:24:50,237 - [INFO    ] - Created tokenizer with init 2487 tokens.\n",
      "2021-08-18 10:24:50,279 - [INFO    ] - [FoodiML] Loaded 5608 images and 5608 annotations.\n",
      "2021-08-18 10:24:50,283 - [INFO    ] - Loaded vocab containing 2487 tokens\n",
      "2021-08-18 10:24:50,283 - [INFO    ] - Loaded from .vocab_cache/foodiml_vocab.json.\n",
      "2021-08-18 10:24:50,284 - [INFO    ] - Created tokenizer with init 2487 tokens.\n",
      "2021-08-18 10:24:50,323 - [INFO    ] - [FoodiML] Loaded 2403 images and 2403 annotations.\n",
      "2021-08-18 10:24:50,326 - [INFO    ] - Adapt loaders: 0\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"DATA_PATH\"] = \"/home/ec2-user/SageMaker/data/\"\n",
    "\n",
    "options = \"options/adapt/foodi-ml/i2t.yaml\"\n",
    "\n",
    "args = {\"options\": options}\n",
    "args = Dict(args)\n",
    "opt = load_yaml_opts(args.options)\n",
    "\n",
    "logger = create_logger(level='debug' if opt.engine.debug else 'info')\n",
    "\n",
    "# Get path of the data\n",
    "data_path = get_data_path(opt)\n",
    "\n",
    "# Get loaders\n",
    "train_loader, val_loaders, adapt_loaders = get_loaders(data_path, args.local_rank, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43e714a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers = get_tokenizers(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0658675e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-18 10:24:52,108 - [INFO    ] - Image encoder created: ('full_image',)\n",
      "2021-08-18 10:24:52,400 - [INFO    ] - Text encoder created: gru_glove\n",
      "2021-08-18 10:24:52,470 - [INFO    ] - Created similarity: AdaptiveEmbeddingI2T(\n",
      "  (norm): Normalization(\n",
      "    (norm): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  )\n",
      "  (adapt_txt): ADAPT(\n",
      "    (fc_gamma): Sequential(\n",
      "      (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    )\n",
      "    (fc_beta): Sequential(\n",
      "      (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (fovea): Fovea(smooth=10,train_smooth: False)\n",
      ")\n",
      "2021-08-18 10:24:55,431 - [INFO    ] - Setting devices: img: cuda,txt: cuda, loss: cuda\n",
      "2021-08-18 10:24:55,431 - [INFO    ] - Using similarity: ('adapt_i2t',)\n"
     ]
    }
   ],
   "source": [
    "model = Retrieval(**opt.model, tokenizers=tokenizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa49266d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method tqdm.write of <class 'tqdm._tqdm.tqdm'>>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_fn = (lambda x: x) if not model.master else tqdm.write\n",
    "print_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34159cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_criterion(opt, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f08f946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer\n",
    "trainer = train.Trainer(\n",
    "    model=model,\n",
    "    args=opt,\n",
    "    sysoutlog=print_fn,\n",
    "    path=opt.exp.outpath,\n",
    "    world_size=1 # TODO\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "539709e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-18 10:24:55,463 - [INFO    ] - lr 0.001\n",
      "2021-08-18 10:24:55,464 - [INFO    ] - [0.5, 2.0, 4000]\n",
      "2021-08-18 10:24:55,464 - [INFO    ] - [10000, 20000, 3000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing model.txt_enc.embed.glove\n",
      "lr: 0.001, #layers: 478, #params: 99,845,812\n",
      "Total Params: 102,349,912, \n"
     ]
    }
   ],
   "source": [
    "trainer.setup_optim(\n",
    "        lr=opt.optimizer.lr,\n",
    "        lr_scheduler=opt.optimizer.lr_scheduler,\n",
    "        clip_grad=opt.optimizer.grad_clip,\n",
    "        log_grad_norm=False,\n",
    "        log_histograms=False,\n",
    "        optimizer=opt.optimizer,\n",
    "        freeze_modules=opt.model.freeze_modules\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6f879d",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e4a9e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.save(\n",
    "#    path = \"runs\",\n",
    "#    is_best = True,\n",
    "#    epoch = 0,\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9213f7",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ab7a307",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bf4613",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9ae6ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader=train_loader\n",
    "valid_loaders=val_loaders\n",
    "lang_loaders=adapt_loaders\n",
    "nb_epochs=opt.engine.nb_epochs\n",
    "valid_interval=opt.engine.valid_interval\n",
    "log_interval=opt.engine.print_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ab5ac89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7f957f351208>\n",
      "[<torch.utils.data.dataloader.DataLoader object at 0x7f957e9aa278>]\n",
      "[]\n",
      "1\n",
      "500\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(train_loader)\n",
    "print(valid_loaders)\n",
    "print(lang_loaders)\n",
    "print(nb_epochs)\n",
    "print(valid_interval)\n",
    "print(log_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cef2df9",
   "metadata": {},
   "source": [
    "# Train epoch(Deep dive) -------------------------------- START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c073372c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader\n",
    "lang_loaders\n",
    "epoch = 0\n",
    "valid_loaders=val_loaders\n",
    "log_interval=50\n",
    "valid_interval=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08266c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26fcfc9",
   "metadata": {},
   "source": [
    "## Run Evaluation (Deep Dive) ----------------- START"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc73c4b3",
   "metadata": {},
   "source": [
    "### Evaluate Loaders (Deep Dive) ----------------- START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c5cda594",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dep dive evaluate_loaders\n",
    "# metrics, val_metric = self.evaluate_loaders(valid_loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3079ca2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = valid_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "08d08a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_metrics = {}\n",
    "final_sum = 0.\n",
    "nb_loaders = len(loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6d895368",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = loaders[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "115ceb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_name = str(loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05326f05",
   "metadata": {},
   "source": [
    "#### Predict Loader (Deep Dive) ----------------- START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4279b986",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Deep dive predict_loader\n",
    "# img_emb, txt_emb, lens = evaluation.predict_loader(self.model, loader, self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5978c7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arguments\n",
    "model = trainer.model\n",
    "data_loader = loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3bd9685b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "img_embs, cap_embs, cap_lens = None, None, None\n",
    "max_n_word = 77\n",
    "model.eval()\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c1c959e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "genload = iter(data_loader)\n",
    "batch = next(genload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "66134e5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch['caption'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "928bd6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = batch['index']\n",
    "cap, lengths = batch['caption']\n",
    "img_emb, cap_emb = model.forward_batch(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "91ee848a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ff232c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2048, 49])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8b892784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 36, 2048])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6bdd0f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_tensor = True\n",
    "img_embs = np.zeros((len(data_loader.dataset), img_emb.size(1), img_emb.size(2)))\n",
    "cap_embs = np.zeros((len(data_loader.dataset), max_n_word, cap_emb.size(2)))\n",
    "cap_lens = [0] * len(data_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0f9f642c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_n_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5d1d87ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2403"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cap_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "dee52440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2403, 2048, 49)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0653b3ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2403, 77, 2048)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1ca60974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache embeddings\n",
    "img_embs[ids] = img_emb.data.cpu().numpy()\n",
    "if is_tensor:\n",
    "    cap_embs[ids,:max(lengths),:] = cap_emb.data.cpu().numpy()\n",
    "else:\n",
    "    cap_embs[ids,] = cap_emb.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4faa0b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2403, 2048, 49)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "50d3dc1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2403, 77, 2048)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "019a2e7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2403"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cap_lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9794199",
   "metadata": {},
   "source": [
    "#### Predict Loader (Deep Dive) ----------------- END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a1281e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from retrieval.train.evaluation import predict_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "10f1b44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    }
   ],
   "source": [
    "img_emb, txt_emb, lens = predict_loader(trainer.model, loader, trainer.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7d3a5a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2403, 2048, 49)\n",
      "(2403, 77, 2048)\n",
      "2403\n"
     ]
    }
   ],
   "source": [
    "print(img_emb.shape)\n",
    "print(txt_emb.shape)\n",
    "print(len(lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c057f1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from retrieval.train.evaluation import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf9e8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test  :  74%|███████▎  | 14/19 [02:53<01:01, 12.38s/it]"
     ]
    }
   ],
   "source": [
    "result = evaluate(\n",
    "    model=trainer.model, \n",
    "    img_emb=img_emb,\n",
    "    txt_emb=txt_emb, \n",
    "    lengths=lens,\n",
    "    device=trainer.device, \n",
    "    shared_size=128\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7641beef",
   "metadata": {},
   "source": [
    "#### Evaluate (Deep Dive) ----------------- START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3025ffad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trainer.model\n",
    "img_emb = img_emb\n",
    "txt_emb = txt_emb\n",
    "lengths = lens\n",
    "device = trainer.device\n",
    "shared_size=128\n",
    "return_sims=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74aeb485",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54ca371",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bcde9a8e",
   "metadata": {},
   "source": [
    "#### Evaluate (Deep Dive) ----------------- END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5f67bf",
   "metadata": {},
   "source": [
    "### Evaluate Loaders (Deep Dive) ----------------- END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d091a857",
   "metadata": {},
   "source": [
    "### Run Evaluation (Deep Dive) ----------------- END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0126a43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f666582d",
   "metadata": {},
   "source": [
    "## Train epoch(Deep dive) -------------------------------- END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "089dbfb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57423a93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

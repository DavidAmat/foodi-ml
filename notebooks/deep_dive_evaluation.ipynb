{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efe0ebc6",
   "metadata": {},
   "source": [
    "# Deep Dive Evaluation during Training\n",
    "\n",
    "Execution \n",
    "\n",
    "```{bash}\n",
    "cd /home/ec2-user/SageMaker/foodi-ml\n",
    "source activate python3\n",
    "export DATA_PATH=/home/ec2-user/SageMaker/data/\n",
    "python run.py options/adapt/foodi-ml/i2t.yaml\n",
    "\n",
    "#nvidia-smi\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0523820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/ec2-user/SageMaker/foodi-ml/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ccbf7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from addict import Dict\n",
    "\n",
    "import params\n",
    "from retrieval.train import train\n",
    "from retrieval.utils import helper\n",
    "from retrieval.model import loss\n",
    "from retrieval.model.model import Retrieval\n",
    "from retrieval.data.loaders import get_loaders\n",
    "from retrieval.utils.logger import create_logger\n",
    "from retrieval.utils.helper import load_model\n",
    "from retrieval.utils.file_utils import load_yaml_opts, parse_loader_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cfb1cd",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "785fbe34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_path(opt):\n",
    "    if 'DATA_PATH' not in os.environ:\n",
    "        if not opt.dataset.data_path:\n",
    "            raise Exception('''\n",
    "                DATA_PATH not specified.\n",
    "                Please, run \"$ export DATA_PATH=/path/to/dataset\"\n",
    "                or add path to yaml file\n",
    "            ''')\n",
    "        return opt.dataset.data_path\n",
    "    else:\n",
    "        return os.environ['DATA_PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08be6033",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizers(train_loader):\n",
    "    tokenizers = train_loader.dataset.tokenizer\n",
    "    if type(tokenizers) != list:\n",
    "        tokenizers = [tokenizers]\n",
    "    return tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec7b1ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_criterion(opt, model):\n",
    "    if 'name' in opt.criterion:\n",
    "        logger.info(opt.criterion)\n",
    "        multimodal_criterion = loss.get_loss(**opt.criterion)\n",
    "        multilanguage_criterion = loss.get_loss(**opt.criterion)\n",
    "    else:\n",
    "        multimodal_criterion = loss.ContrastiveLoss(**opt.criterion)\n",
    "        multilanguage_criterion = loss.ContrastiveLoss(**opt.ml_criterion)\n",
    "    set_model_criterion(opt, model, multilanguage_criterion, multimodal_criterion)\n",
    "    # return multimodal_criterion, multilanguage_criterion\n",
    "\n",
    "\n",
    "def set_model_criterion(opt, model, multilanguage_criterion, multimodal_criterion):\n",
    "    model.mm_criterion = multimodal_criterion\n",
    "    model.ml_criterion = None\n",
    "    if len(opt.dataset.adapt.data) > 0:\n",
    "        model.ml_criterion = multilanguage_criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f0884c",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee71ea5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-18 11:34:59,924 - [INFO    ] - Loaded vocab containing 2487 tokens\n",
      "2021-08-18 11:34:59,924 - [INFO    ] - Loaded from .vocab_cache/foodiml_vocab.json.\n",
      "2021-08-18 11:34:59,925 - [INFO    ] - Created tokenizer with init 2487 tokens.\n",
      "2021-08-18 11:34:59,967 - [INFO    ] - [FoodiML] Loaded 5608 images and 5608 annotations.\n",
      "2021-08-18 11:34:59,971 - [INFO    ] - Loaded vocab containing 2487 tokens\n",
      "2021-08-18 11:34:59,971 - [INFO    ] - Loaded from .vocab_cache/foodiml_vocab.json.\n",
      "2021-08-18 11:34:59,972 - [INFO    ] - Created tokenizer with init 2487 tokens.\n",
      "2021-08-18 11:35:00,010 - [INFO    ] - [FoodiML] Loaded 2403 images and 2403 annotations.\n",
      "2021-08-18 11:35:00,013 - [INFO    ] - Adapt loaders: 0\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"DATA_PATH\"] = \"/home/ec2-user/SageMaker/data/\"\n",
    "\n",
    "options = \"options/adapt/foodi-ml/i2t.yaml\"\n",
    "\n",
    "args = {\"options\": options}\n",
    "args = Dict(args)\n",
    "opt = load_yaml_opts(args.options)\n",
    "\n",
    "logger = create_logger(level='debug' if opt.engine.debug else 'info')\n",
    "\n",
    "# Get path of the data\n",
    "data_path = get_data_path(opt)\n",
    "\n",
    "# Get loaders\n",
    "train_loader, val_loaders, adapt_loaders = get_loaders(data_path, args.local_rank, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "611bc3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers = get_tokenizers(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91b1b958",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-18 11:35:02,507 - [INFO    ] - Image encoder created: ('full_image',)\n",
      "2021-08-18 11:35:02,800 - [INFO    ] - Text encoder created: gru_glove\n",
      "2021-08-18 11:35:02,871 - [INFO    ] - Created similarity: AdaptiveEmbeddingI2T(\n",
      "  (norm): Normalization(\n",
      "    (norm): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  )\n",
      "  (adapt_txt): ADAPT(\n",
      "    (fc_gamma): Sequential(\n",
      "      (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    )\n",
      "    (fc_beta): Sequential(\n",
      "      (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (fovea): Fovea(smooth=10,train_smooth: False)\n",
      ")\n",
      "2021-08-18 11:35:05,829 - [INFO    ] - Setting devices: img: cuda,txt: cuda, loss: cuda\n",
      "2021-08-18 11:35:05,829 - [INFO    ] - Using similarity: ('adapt_i2t',)\n"
     ]
    }
   ],
   "source": [
    "model = Retrieval(**opt.model, tokenizers=tokenizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ffddfe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method tqdm.write of <class 'tqdm._tqdm.tqdm'>>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_fn = (lambda x: x) if not model.master else tqdm.write\n",
    "print_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cd47f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_criterion(opt, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7f78a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer\n",
    "trainer = train.Trainer(\n",
    "    model=model,\n",
    "    args=opt,\n",
    "    sysoutlog=print_fn,\n",
    "    path=opt.exp.outpath,\n",
    "    world_size=1 # TODO\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "125146d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-18 11:17:28,445 - [INFO    ] - lr 0.001\n",
      "2021-08-18 11:17:28,446 - [INFO    ] - [0.5, 2.0, 4000]\n",
      "2021-08-18 11:17:28,446 - [INFO    ] - [10000, 20000, 3000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing model.txt_enc.embed.glove\n",
      "lr: 0.001, #layers: 478, #params: 99,845,812\n",
      "Total Params: 102,349,912, \n"
     ]
    }
   ],
   "source": [
    "trainer.setup_optim(\n",
    "        lr=opt.optimizer.lr,\n",
    "        lr_scheduler=opt.optimizer.lr_scheduler,\n",
    "        clip_grad=opt.optimizer.grad_clip,\n",
    "        log_grad_norm=False,\n",
    "        log_histograms=False,\n",
    "        optimizer=opt.optimizer,\n",
    "        freeze_modules=opt.model.freeze_modules\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54546eab",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ba8878e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.save(\n",
    "#    path = \"runs\",\n",
    "#    is_best = True,\n",
    "#    epoch = 0,\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806d7dd3",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5fbb8523",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fa9891",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9620ed47",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader=train_loader\n",
    "valid_loaders=val_loaders\n",
    "lang_loaders=adapt_loaders\n",
    "nb_epochs=opt.engine.nb_epochs\n",
    "valid_interval=opt.engine.valid_interval\n",
    "log_interval=opt.engine.print_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "542dafe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7f2f19473128>\n",
      "[<torch.utils.data.dataloader.DataLoader object at 0x7f2f17a0efd0>]\n",
      "[]\n",
      "1\n",
      "500\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(train_loader)\n",
    "print(valid_loaders)\n",
    "print(lang_loaders)\n",
    "print(nb_epochs)\n",
    "print(valid_interval)\n",
    "print(log_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5c8949",
   "metadata": {},
   "source": [
    "# Train epoch(Deep dive) -------------------------------- START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ceecd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader\n",
    "lang_loaders\n",
    "epoch = 0\n",
    "valid_loaders=val_loaders\n",
    "log_interval=50\n",
    "valid_interval=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57ba85fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512f8aca",
   "metadata": {},
   "source": [
    "## Run Evaluation (Deep Dive) ----------------- START"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a01966",
   "metadata": {},
   "source": [
    "### Evaluate Loaders (Deep Dive) ----------------- START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "94abbd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dep dive evaluate_loaders\n",
    "# metrics, val_metric = self.evaluate_loaders(valid_loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce225d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = valid_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "37736586",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_metrics = {}\n",
    "final_sum = 0.\n",
    "nb_loaders = len(loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5f742bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = loaders[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5954355d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_name = str(loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bf949d",
   "metadata": {},
   "source": [
    "#### Predict Loader (Deep Dive) ----------------- START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1ba641b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Deep dive predict_loader\n",
    "# img_emb, txt_emb, lens = evaluation.predict_loader(self.model, loader, self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "69be4711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arguments\n",
    "model = trainer.model\n",
    "data_loader = loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1071afc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "img_embs, cap_embs, cap_lens = None, None, None\n",
    "max_n_word = 77\n",
    "model.eval()\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b4c25914",
   "metadata": {},
   "outputs": [],
   "source": [
    "genload = iter(data_loader)\n",
    "batch = next(genload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8331f5b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch['caption'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ee906901",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = batch['index']\n",
    "cap, lengths = batch['caption']\n",
    "img_emb, cap_emb = model.forward_batch(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1e6f55b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2197, 4367, 3558, 5274, 2360, 3384, 5528,  636, 3013, 1841, 4718,\n",
       "       4909,  348, 1521, 1983, 4447, 1248, 1648, 1692, 4815, 1114, 3026,\n",
       "        615, 1953, 3752, 5025,  460, 4635, 3327,  922, 3286,  737])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b455945c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2048, 49])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "aa172665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 36, 2048])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f7c99777",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_tensor = True\n",
    "img_embs = np.zeros((len(data_loader.dataset), img_emb.size(1), img_emb.size(2)))\n",
    "cap_embs = np.zeros((len(data_loader.dataset), max_n_word, cap_emb.size(2)))\n",
    "cap_lens = [0] * len(data_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0e94666f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_n_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "16a820e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2403"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cap_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e5a1f917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2403, 2048, 49)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b9ebb9dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2403, 77, 2048)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e089a4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache embeddings\n",
    "img_embs[ids] = img_emb.data.cpu().numpy()\n",
    "if is_tensor:\n",
    "    cap_embs[ids,:max(lengths),:] = cap_emb.data.cpu().numpy()\n",
    "else:\n",
    "    cap_embs[ids,] = cap_emb.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4df10449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2403, 2048, 49)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "93e58ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2403, 77, 2048)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4392fc16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2403"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cap_lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d34b227",
   "metadata": {},
   "source": [
    "#### Predict Loader (Deep Dive) ----------------- END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a6594e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from retrieval.train.evaluation import predict_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e1338ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    }
   ],
   "source": [
    "img_emb, txt_emb, lens = predict_loader(trainer.model, loader, trainer.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b5a7e801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2403, 2048, 49)\n",
      "(2403, 77, 2048)\n",
      "2403\n"
     ]
    }
   ],
   "source": [
    "print(img_emb.shape)\n",
    "print(txt_emb.shape)\n",
    "print(len(lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6f7f9ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from retrieval.train.evaluation import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4576bb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    }
   ],
   "source": [
    "result = evaluate(\n",
    "    model=trainer.model, \n",
    "    img_emb=img_emb,\n",
    "    txt_emb=txt_emb, \n",
    "    lengths=lens,\n",
    "    device=trainer.device, \n",
    "    shared_size=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3df2df01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pred_time': 1.3503006880000612,\n",
       " 'sim_time': 233.0349687279995,\n",
       " 'i2t_r1': 0.08322929671244278,\n",
       " 'i2t_r5': 0.16645859342488556,\n",
       " 'i2t_r10': 0.4577611319184353,\n",
       " 'i2t_medr': 1205.0,\n",
       " 'i2t_meanr': 1200.3354140657511,\n",
       " 't2i_r1': 0.0,\n",
       " 't2i_r5': 0.20807324178110695,\n",
       " 't2i_r10': 0.4577611319184353,\n",
       " 't2i_medr': 1187.0,\n",
       " 't2i_meanr': 1197.8322929671244,\n",
       " 'rsum': 1.373283395755306}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273da5b6",
   "metadata": {},
   "source": [
    "#### Evaluate (Deep Dive) ----------------- START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6974729a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trainer.model\n",
    "img_emb = img_emb\n",
    "txt_emb = txt_emb\n",
    "lengths = lens\n",
    "device = trainer.device\n",
    "shared_size=128\n",
    "return_sims=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "898c79d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "_metrics_ = ('r1', 'r5', 'r10', 'medr', 'meanr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "25c9addc",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_emb = torch.FloatTensor(img_emb).to(device)\n",
    "txt_emb = torch.FloatTensor(txt_emb).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7d67c78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = np.array(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b914eb10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2408e7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(img_emb, 'tmp/imb_emb.pt')\n",
    "#torch.save(txt_emb, 'tmp/txt_emb.pt')\n",
    "#np.save('tmp/lengths', lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91d6d334",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_emb = torch.load('tmp/imb_emb.pt')\n",
    "txt_emb = torch.load('tmp/txt_emb.pt')\n",
    "lengths = list(np.load('tmp/lengths.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a53b232c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 2048, 49])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f9a143a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_emb = img_emb[:12,:,:]\n",
    "txt_emb = txt_emb[:12,:,:]\n",
    "lengths = lengths[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a726d97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.84 ms, sys: 9.34 ms, total: 19.2 ms\n",
      "Wall time: 18.1 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sims = model.get_sim_matrix_shared(\n",
    "    embed_a=img_emb, \n",
    "    embed_b=txt_emb,\n",
    "    lens=lengths, \n",
    "    shared_size=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b344339",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = model.similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "63788967",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_a=img_emb\n",
    "embed_b=txt_emb\n",
    "lens=lengths,\n",
    "shared_size=12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b85c8ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_embed = embed_a\n",
    "cap_embed = embed_b\n",
    "lens=lengths\n",
    "shared_size=shared_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "144387f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_im_shard = (len(img_embed)-1)//shared_size + 1\n",
    "n_cap_shard = (len(cap_embed)-1)//shared_size + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f17a7e12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_cap_shard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9062d189",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = torch.zeros(len(img_embed), len(cap_embed)).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6d4237c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "75f0589e",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "j = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ec830a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_start = shared_size*i\n",
    "im_end = min(shared_size*(i+1), len(img_embed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a498993e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e439579b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "59a606f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_start = shared_size*j\n",
    "cap_end = min(shared_size*(j+1), len(cap_embed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "85285d96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c64e43f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a888dfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = img_embed[im_start:im_end]\n",
    "s = cap_embed[cap_start:cap_end]\n",
    "l = lens[cap_start:cap_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d16de3db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 2048, 49])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "09cdaa10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 77, 2048])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "785f8a1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d8a6dd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = model.similarity.forward(im, s, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f81d2cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 12])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim.shape\n",
    "# rows are image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5ed3c2f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 12])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bed822e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim2 = model.similarity.forward(img_embed, cap_embed, lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "981cf95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from retrieval.utils.layers import tensor_to_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ee65bf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = tensor_to_numpy(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2fb56b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 12)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976b6e86",
   "metadata": {},
   "source": [
    "##### Image 2 Text metrics (Deep Dive) --- START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b0d575da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#i2t_metrics = i2t(sims)\n",
    "#t2i_metrics = t2i(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4e30b47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "npts, ncaps = sim.shape\n",
    "captions_per_image = ncaps // npts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bf6cbbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = np.zeros(npts)\n",
    "top1 = np.zeros(npts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a3b65bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "3e99b139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4724, 0.5718, 0.5298, 0.5168, 0.5247, 0.5420, 0.5529, 0.4668, 0.5212,\n",
       "        0.5628, 0.5902, 0.5679], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similarity of a given image embedding, to all of the caption embeddings of validation\n",
    "sims[index] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "2dc6d047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10,  1, 11,  9,  6,  5,  2,  4,  8,  3,  0,  7])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The first position of inds is the position of the sims[index] vector of the HIGHEST similar\n",
    "inds = np.argsort(sim[index])[::-1]\n",
    "inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc93b0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ideally, we would want that the first position of the inds is 0. \n",
    "# That is why we apply the \"where\" to search for the position of the 0\n",
    "# in the inds array. This position is the \"rank\", ideally the lower the better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "fa01a191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 0, 1, 2, 3])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ascendingly\n",
    "np.argsort([1,3,4,5,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "36c97c6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 1, 0, 4])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Descendingly\n",
    "np.argsort([1,3,4,5,0])[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b18a0857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1)\n"
     ]
    }
   ],
   "source": [
    "# Score\n",
    "rank = 1e20\n",
    "begin = captions_per_image * index\n",
    "end = captions_per_image * index + captions_per_image\n",
    "print((begin, end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a2a6b104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for the position of the inds = 0\n",
    "for i in range(begin, end, 1):\n",
    "    # Check the \"diagonal\" element of inds, indicating the rank of the\n",
    "    # TRUE embedding that matched the query embedding index. We ideally want\n",
    "    # this rank\n",
    "    tmp = np.where(inds == i)[0][0]\n",
    "    if tmp < rank:\n",
    "        rank = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d680f6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks[index] = rank\n",
    "top1[index] = inds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4fd036a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10.,  1.,  6.,  9.,  7.,  4.,  5., 11.,  9.,  2.,  0.,  1.])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4c5398c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "npts, ncaps = sim.shape\n",
    "captions_per_image = ncaps // npts\n",
    "\n",
    "ranks = np.zeros(npts)\n",
    "top1 = np.zeros(npts)\n",
    "for index in range(npts):\n",
    "    inds = np.argsort(sim[index])[::-1]\n",
    "    # Score\n",
    "    rank = 1e20\n",
    "    begin = captions_per_image * index\n",
    "    end = captions_per_image * index + captions_per_image\n",
    "    for i in range(begin, end, 1):\n",
    "        # tell me which is the position\n",
    "        tmp = np.where(inds == i)[0][0]\n",
    "        if tmp < rank:\n",
    "            rank = tmp\n",
    "    ranks[index] = rank\n",
    "    top1[index] = inds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7360fa1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10.,  1.,  6.,  9.,  7.,  4.,  5., 11.,  9.,  2.,  0.,  1.])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "dce747ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for image embedding 0, the most similar one is caption embedding 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9795c456",
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = np.round(100.0 * len(np.where(ranks < 1)[0]) / len(ranks),2)\n",
    "r5 = np.round(100.0 * len(np.where(ranks < 5)[0]) / len(ranks),2)\n",
    "r10 = np.round(100.0 * len(np.where(ranks < 10)[0]) / len(ranks),2)\n",
    "medr = np.round(np.floor(np.median(ranks)) + 1,2)\n",
    "meanr = np.round(ranks.mean() + 1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b3db4c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r1 8.33\n",
      "r5 41.67\n",
      "r10 83.33\n",
      "medr 6.0\n",
      "meanr 6.42\n"
     ]
    }
   ],
   "source": [
    "# Compute metrics\n",
    "print('r1', np.round(r1,2))\n",
    "print('r5', r5)\n",
    "print('r10', r10)\n",
    "print('medr', medr)\n",
    "print('meanr', meanr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f1429b",
   "metadata": {},
   "source": [
    "##### Image 2 Text metrics (Deep Dive) --- END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "89e84a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from retrieval.train.evaluation import t2i, i2t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "49e4eb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2i_metrics = t2i(sim)\n",
    "i2t_metrics = i2t(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "545a28c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8.333333333333334, 33.333333333333336, 75.0, 7.0, 7.083333333333333)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2i_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ba0f07b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8.333333333333334,\n",
       " 41.666666666666664,\n",
       " 83.33333333333333,\n",
       " 6.0,\n",
       " 6.416666666666667)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i2t_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "b6d04f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "rsum = np.sum(i2t_metrics[:3]) + np.sum(t2i_metrics[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "9941af1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250.0"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "f15aff68",
   "metadata": {},
   "outputs": [],
   "source": [
    "_metrics_ = ('r1', 'r5', 'r10', 'medr', 'meanr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "743f72f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "i2t_metrics = {f'i2t_{k}': v for k, v in zip(_metrics_, i2t_metrics)}\n",
    "t2i_metrics = {f't2i_{k}': v for k, v in zip(_metrics_, t2i_metrics)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "41ad2322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i2t_r1': 8.333333333333334,\n",
       " 'i2t_r5': 41.666666666666664,\n",
       " 'i2t_r10': 83.33333333333333,\n",
       " 'i2t_medr': 6.0,\n",
       " 'i2t_meanr': 6.416666666666667}"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i2t_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6707dae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d78547",
   "metadata": {},
   "source": [
    "#### Evaluate (Deep Dive) ----------------- END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "fe05c192",
   "metadata": {},
   "outputs": [],
   "source": [
    "#result = evaluation.evaluate(\n",
    "#                model=self.model, img_emb=img_emb,\n",
    "#                txt_emb=txt_emb, lengths=lens,\n",
    "#                device=self.device, shared_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c36b4f",
   "metadata": {},
   "source": [
    "### Evaluate Loaders (Deep Dive) ----------------- END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab5b266",
   "metadata": {},
   "source": [
    "### Run Evaluation (Deep Dive) ----------------- END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a721e25e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c946e16",
   "metadata": {},
   "source": [
    "## Train epoch(Deep dive) -------------------------------- END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c2d0a3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4079a5f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

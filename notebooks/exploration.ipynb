{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b75cd0c0",
   "metadata": {},
   "source": [
    "# Exploration of execution\n",
    "\n",
    "Execution \n",
    "\n",
    "```{bash}\n",
    "cd /home/ec2-user/SageMaker/foodi-ml\n",
    "source activate python3\n",
    "export DATA_PATH=/home/ec2-user/SageMaker/data/\n",
    "python run.py options/adapt/foodi-ml/i2t.yaml\n",
    "\n",
    "#watch -n 1 \"nvidia-smi\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a392d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/ec2-user/SageMaker/foodi-ml/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2de24b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install https://download.pytorch.org/whl/cu100/torch-1.1.0-cp36-cp36m-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61043244",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d996c9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from addict import Dict\n",
    "\n",
    "import params\n",
    "from retrieval.train import train\n",
    "from retrieval.utils import helper\n",
    "from retrieval.model import loss\n",
    "from retrieval.model.model import Retrieval\n",
    "from retrieval.data.loaders import get_loaders\n",
    "from retrieval.utils.logger import create_logger\n",
    "from retrieval.utils.helper import load_model\n",
    "from retrieval.utils.file_utils import load_yaml_opts, parse_loader_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e6d181",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86e73bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_path(opt):\n",
    "    if 'DATA_PATH' not in os.environ:\n",
    "        if not opt.dataset.data_path:\n",
    "            raise Exception('''\n",
    "                DATA_PATH not specified.\n",
    "                Please, run \"$ export DATA_PATH=/path/to/dataset\"\n",
    "                or add path to yaml file\n",
    "            ''')\n",
    "        return opt.dataset.data_path\n",
    "    else:\n",
    "        return os.environ['DATA_PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fc9a2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizers(train_loader):\n",
    "    tokenizers = train_loader.dataset.tokenizer\n",
    "    if type(tokenizers) != list:\n",
    "        tokenizers = [tokenizers]\n",
    "    return tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7688266e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_criterion(opt, model):\n",
    "    if 'name' in opt.criterion:\n",
    "        logger.info(opt.criterion)\n",
    "        multimodal_criterion = loss.get_loss(**opt.criterion)\n",
    "        multilanguage_criterion = loss.get_loss(**opt.criterion)\n",
    "    else:\n",
    "        multimodal_criterion = loss.ContrastiveLoss(**opt.criterion)\n",
    "        multilanguage_criterion = loss.ContrastiveLoss(**opt.ml_criterion)\n",
    "    set_model_criterion(opt, model, multilanguage_criterion, multimodal_criterion)\n",
    "    # return multimodal_criterion, multilanguage_criterion\n",
    "\n",
    "\n",
    "def set_model_criterion(opt, model, multilanguage_criterion, multimodal_criterion):\n",
    "    model.mm_criterion = multimodal_criterion\n",
    "    model.ml_criterion = None\n",
    "    if len(opt.dataset.adapt.data) > 0:\n",
    "        model.ml_criterion = multilanguage_criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e66feb4",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0391e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"DATA_PATH\"] = \"/home/ec2-user/SageMaker/dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c176bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = \"options/adapt/foodi-ml/i2t.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "431e48f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"options\": options,\n",
    "}\n",
    "args = Dict(args)\n",
    "opt = load_yaml_opts(args.options)\n",
    "opt['operation_name']='test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ac3d131",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = create_logger(level='debug' if opt.engine.debug else 'info')\n",
    "#logger.info(f'Used args   : \\n{args}')\n",
    "#logger.info(f'Used options: \\n{opt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ccab3f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get path of the data\n",
    "data_path = get_data_path(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0311dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-27 15:17:49,599 - [INFO    ] - Loaded vocab containing 4963 tokens\n",
      "2021-09-27 15:17:49,599 - [INFO    ] - Loaded from .vocab_cache/foodiml_vocab.json.\n",
      "2021-09-27 15:17:49,600 - [INFO    ] - Created tokenizer with init 4963 tokens.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self._data_path:  /home/ec2-user/SageMaker/dataset\n",
      "foodiml\n",
      "self.full_path:  /home/ec2-user/SageMaker/dataset/foodiml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-27 15:17:50,467 - [INFO    ] - [FoodiML] Loaded 7000 images annotated \n",
      "2021-09-27 15:17:50,473 - [INFO    ] - Loaded vocab containing 4963 tokens\n",
      "2021-09-27 15:17:50,473 - [INFO    ] - Loaded from .vocab_cache/foodiml_vocab.json.\n",
      "2021-09-27 15:17:50,474 - [INFO    ] - Created tokenizer with init 4963 tokens.\n",
      "2021-09-27 15:17:50,595 - [INFO    ] - [FoodiML] Loaded 1500 images annotated \n",
      "2021-09-27 15:17:50,596 - [INFO    ] - Adapt loaders: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self._data_path:  /home/ec2-user/SageMaker/dataset\n",
      "foodiml\n",
      "self.full_path:  /home/ec2-user/SageMaker/dataset/foodiml\n"
     ]
    }
   ],
   "source": [
    "# Get loaders\n",
    "train_loader, val_loaders, adapt_loaders = get_loaders(data_path, args.local_rank, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ae8e5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers = get_tokenizers(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "394b2f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet152-b121ed2d.pth\" to /home/ec2-user/.cache/torch/checkpoints/resnet152-b121ed2d.pth\n",
      "100%|██████████| 241530880/241530880 [00:05<00:00, 46880215.40it/s]\n",
      "2021-09-27 15:18:21,251 - [INFO    ] - Image encoder created: ('full_image',)\n",
      "2021-09-27 15:18:21,537 - [INFO    ] - Text encoder created: gru_glove\n",
      "2021-09-27 15:18:21,667 - [INFO    ] - Created similarity: AdaptiveEmbeddingI2T(\n",
      "  (norm): Normalization(\n",
      "    (norm): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  )\n",
      "  (adapt_txt): ADAPT(\n",
      "    (fc_gamma): Sequential(\n",
      "      (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    )\n",
      "    (fc_beta): Sequential(\n",
      "      (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (fovea): Fovea(smooth=10,train_smooth: False)\n",
      ")\n",
      "2021-09-27 15:18:21,668 - [INFO    ] - Created similarity: AdaptiveEmbeddingI2T_eval(\n",
      "  (norm): Normalization(\n",
      "    (norm): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  )\n",
      "  (adapt_txt): ADAPT(\n",
      "    (fc_gamma): Sequential(\n",
      "      (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    )\n",
      "    (fc_beta): Sequential(\n",
      "      (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (fovea): Fovea(smooth=10,train_smooth: False)\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new similarity class initialised\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-27 15:18:26,558 - [INFO    ] - Setting devices: img: cuda,txt: cuda, loss: cuda\n",
      "2021-09-27 15:18:26,558 - [INFO    ] - Using similarity: ('adapt_i2t',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.35 s, sys: 1.82 s, total: 6.17 s\n",
      "Wall time: 12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = Retrieval(**opt.model, tokenizers=tokenizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "830b3a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_fn = (lambda x: x) if not model.master else tqdm.write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77d61e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_criterion(opt, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9644a018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer\n",
    "trainer = train.Trainer(\n",
    "        model=model,\n",
    "        args=opt,\n",
    "        sysoutlog=print_fn,\n",
    "        path=opt.exp.outpath,\n",
    "        world_size=1 # TODO\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7dcb761",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-27 15:18:35,698 - [INFO    ] - lr 0.001\n",
      "2021-09-27 15:18:35,698 - [INFO    ] - [0.5, 2.0, 4000]\n",
      "2021-09-27 15:18:35,699 - [INFO    ] - [10000, 20000, 3000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing model.txt_enc.embed.glove\n",
      "lr: 0.001, #layers: 478, #params: 100,588,612\n",
      "Total Params: 102,077,512, \n"
     ]
    }
   ],
   "source": [
    "trainer.setup_optim(\n",
    "        lr=opt.optimizer.lr,\n",
    "        lr_scheduler=opt.optimizer.lr_scheduler,\n",
    "        clip_grad=opt.optimizer.grad_clip,\n",
    "        log_grad_norm=False,\n",
    "        log_histograms=False,\n",
    "        optimizer=opt.optimizer,\n",
    "        freeze_modules=opt.model.freeze_modules\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e28d99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7f82896fa9b0>\n"
     ]
    }
   ],
   "source": [
    "print(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de6f0941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.engine.valid_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83739700",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "Epochs:   0%|          | 0/10 [00:10<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch_Time:   0.883552, Countdown:  50.000000, Epoch:   0.000000, Iteration:  10.000000, K:   0.086441, Loss: 286.250061, Lr_Base:   0.000504, Norm: 647.066952, Total_Loss: 286.250061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/10 [00:20<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch_Time:   1.167607, Countdown:  50.000000, Epoch:   0.000000, Iteration:  20.000000, K:   0.165410, Loss: 291.062805, Lr_Base:   0.000508, Norm: 715.587118, Total_Loss: 291.062805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/10 [00:30<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch_Time:   0.938989, Countdown:  50.000000, Epoch:   0.000000, Iteration:  30.000000, K:   0.237553, Loss: 233.167374, Lr_Base:   0.000511, Norm: 513.135821, Total_Loss: 233.167374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/10 [00:31<?, ?it/s]\n",
      "Pred  :   0%|          | 0/1500 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_loaders:  1\n",
      "Evaluating  1/ 1 - foodiml.dev.es\n",
      "evaluation.predict_loader begins\n",
      "Evaluation begins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after doing forward in one batch in evaluation: img_emb.size(): torch.Size([1, 2048, 49])\n",
      "after doing forward in one batch in evaluation: cap_emb.size(): torch.Size([1, 27, 2048])\n",
      "Trying to allocate a tensor in CPU of (1500, 2048, 49)\n",
      "New size of the img_embs = (1500, 2048)\n",
      "Declaring giant matrix for captions...\n",
      "Declared\n",
      "dimension of image vector:  torch.Size([1, 2048])\n",
      "dimension of cap_emb:  torch.Size([1, 27, 2048])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (27) must match the size of tensor b (2048) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-fb0f5dc0a185>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mnb_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mvalid_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_interval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mlog_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_freq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;32m~/SageMaker/foodi-ml/retrieval/train/train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_loader, valid_loaders, lang_loaders, init_iteration, nb_epochs, log_interval, valid_interval, world_size)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mvalid_loaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_loaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0mvalid_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_interval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m                 \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m             )\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SageMaker/foodi-ml/retrieval/train/train.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self, train_loader, lang_loaders, epoch, valid_loaders, log_interval, valid_interval, path)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm_criterion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteration\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mvalid_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_loaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm_criterion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteration\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlog_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SageMaker/foodi-ml/retrieval/train/train.py\u001b[0m in \u001b[0;36mrun_evaluation\u001b[0;34m(self, valid_loaders)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_loaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_loaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_early_stop_vars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_metric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SageMaker/foodi-ml/retrieval/train/train.py\u001b[0m in \u001b[0;36mevaluate_loaders\u001b[0;34m(self, loaders)\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msysoutlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Evaluating {i+1:2d}/{nb_loaders:2d} - {loader_name}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"evaluation.predict_loader begins\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             \u001b[0mimg_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtxt_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_loader_smart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"img_emb.size() = \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"txt_emb.size() = \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtxt_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_no_grad\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_no_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_no_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SageMaker/foodi-ml/retrieval/train/evaluation.py\u001b[0m in \u001b[0;36mpredict_loader_smart\u001b[0;34m(model, data_loader, device)\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dimension of cap_emb: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcap_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0mtxt_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madapt_txt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcap_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m             \u001b[0mtxt_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfovea\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mtxt_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtxt_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SageMaker/foodi-ml/retrieval/model/layers/adapt.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, value, query)\u001b[0m\n\u001b[1;32m     93\u001b[0m         )\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mnormalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgammas\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbetas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0mnormalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnormalized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (27) must match the size of tensor b (2048) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "trainer.fit(\n",
    "        train_loader=train_loader,\n",
    "        valid_loaders=val_loaders,\n",
    "        lang_loaders=adapt_loaders,\n",
    "        nb_epochs=opt.engine.nb_epochs,\n",
    "        valid_interval=opt.engine.valid_interval,\n",
    "        log_interval=opt.engine.print_freq\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7276ec15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd0bf1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09e30d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eca5c277",
   "metadata": {},
   "source": [
    "# EXPLORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe7b7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed14b613",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install https://download.pytorch.org/whl/cu100/torch-1.0.1-cp36-cp36m-win_amd64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2e435a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.randn(3, 3, device=\"cuda:0\")\n",
    "a = torch.mm(a, a.t()) # make symmetric positive-definite\n",
    "torch.cholesky(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0983de65",
   "metadata": {},
   "source": [
    "## Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5af57b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "image_id = train_loader.dataset.data_wrapper.image_ids[index]\n",
    "caption = train_loader.dataset.captions[index]\n",
    "cap_tokens = train_loader.dataset.tokenizer(caption)\n",
    "image = train_loader.dataset.load_img(image_id)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(image.permute(1, 2, 0)  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea05d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.max(list(train_loader.dataset.data_wrapper.image_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e97031",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_loader.dataset.captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a275db17",
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1414c200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check valid loader\n",
    "#image_id2 = val_loaders[0].dataset.data_wrapper.image_ids[index]\n",
    "#image2 = val_loaders[0].dataset.load_img(image_id2)\n",
    "#import matplotlib.pyplot as plt\n",
    "#plt.imshow(  image2.permute(1, 2, 0)  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76969e94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de873fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import sys\n",
    "import json\n",
    "import tqdm\n",
    "os.chdir(\"/home/ec2-user/SageMaker/foodi-ml/notebooks/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9367105",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f849ea9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils_aws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97ae798c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils_aws' from '/home/ec2-user/SageMaker/foodi-ml/notebooks/utils_aws.py'>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils_aws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be0e5f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils_aws = reload(utils_aws)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10869ef0",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3816964",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {\n",
    "    \"S3_BUCKET\": 'test-bucket-glovocds',\n",
    "    \"S3K_imgs\": 'artifacts/002/',\n",
    "    \"S3_file_samples\": 'training_data.csv',\n",
    "    \"pth_dwn_samples\": '/home/ec2-user/SageMaker/data/samples',\n",
    "    \"pth_dwn_images\": '/home/ec2-user/SageMaker/data/images',\n",
    "    \"pth_vocab\": '/home/ec2-user/SageMaker/foodi-ml/.vocab_cache/foodiml_vocab.json',\n",
    "    \"pth_dataset_json\": '/home/ec2-user/SageMaker/data/foodiml/dataset_foodiml.json',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdb57f1",
   "metadata": {},
   "source": [
    "# AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d6b05d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS classes\n",
    "aws_con = utils_aws.AWSConnector(conf[\"S3_BUCKET\"])\n",
    "awstools = utils_aws.AWSTools(aws_con)\n",
    "aws_basics = utils_aws.AWSBasics(conf[\"S3_BUCKET\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914518b7",
   "metadata": {},
   "source": [
    "# List cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6392a58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BCN', 'CUG']\n"
     ]
    }
   ],
   "source": [
    "l_cities = awstools.create_list_cities(conf['S3K_imgs'])\n",
    "print(l_cities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb896cd",
   "metadata": {},
   "source": [
    "# Download samples CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bbe4637a",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_cities = ['CUG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e75e17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key artifacts/002/CUG/training_data.csv not found in S3\n",
      "Removing from l_cities city CUG\n"
     ]
    }
   ],
   "source": [
    "l_cities = awstools.downloading_city_csv(\n",
    "    l_cities=l_cities,\n",
    "    s3_key_prefix=conf['S3K_imgs'],\n",
    "    csv_name=conf['S3_file_samples'],\n",
    "    local_folder=conf['pth_dwn_samples'],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "642fa115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'training_data.csv'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf['S3_file_samples']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e12eef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ec2-user/SageMaker/data/samples'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf['pth_dwn_samples']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4e747ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "city = \"CUG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f5e37b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key artifacts/002/CUG/training_data.csv not found in S3\n"
     ]
    }
   ],
   "source": [
    "# Get the prefix of the S3 csv name\n",
    "local_file_name = f\"{city}.csv\"\n",
    "\n",
    "# Get full local path\n",
    "local_file_path = os.path.join(conf['pth_dwn_samples'], local_file_name)\n",
    "\n",
    "# S3 key of the .csv\n",
    "s3k_file_path = os.path.join(conf['S3K_imgs'], city, conf['S3_file_samples'])\n",
    "\n",
    "# Download csv\n",
    "success = aws_basics.download_obj(\n",
    "    s3_key=s3k_file_path,\n",
    "    destination=local_file_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef202e9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "success"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bbe3bb",
   "metadata": {},
   "source": [
    "# Read and concatenate CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89584010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all csv we downloaded\n",
    "l_csv = os.listdir(conf['pth_dwn_samples'])\n",
    "\n",
    "# Impose only .csv\n",
    "l_csv = [file_name for file_name in l_csv if file_name.endswith(\".csv\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38afec2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ec2-user/SageMaker/data/samples'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf['pth_dwn_samples']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12e719cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenation to a single dataframe\n",
    "samples = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36323f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# Read and concatenate each city dataframe to samples\n",
    "for city_csv_file in tqdm.tqdm(l_csv):\n",
    "    path_csv = os.path.join(conf['pth_dwn_samples'], city_csv_file)\n",
    "    df_city_csv = pd.read_csv(path_csv)\n",
    "    df_city_csv.insert(loc=0, \n",
    "                       column=\"city\", \n",
    "                       value = city_csv_file.split(\".csv\")[0])\n",
    "    samples = pd.concat([samples, df_city_csv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "114ab303",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.to_parquet(os.path.join(conf['pth_dwn_samples'], \"samples.parquet\"), engine = \"pyarrow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86012fa",
   "metadata": {},
   "source": [
    "# Download images specified in samples CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7bdf0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = pd.read_parquet(os.path.join(conf['pth_dwn_samples'], \"samples.parquet\"), engine = \"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1acd1596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the folder to dump images\n",
    "img_dwn_paral = utils_aws.ImageDownloaderParallelS3(\n",
    "    base_path=conf['pth_dwn_images']\n",
    ")\n",
    "# Create iterable of jobs and modify img_path column\n",
    "jobs, samples = img_dwn_paral.create_jobs(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0566feb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a process pool to do the work\n",
    "pool = multiprocessing.Pool(\n",
    "    multiprocessing.cpu_count(), \n",
    "    img_dwn_paral.initialize,\n",
    "    (conf['S3_BUCKET'],)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "973642df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48.9 ms, sys: 18.9 ms, total: 67.8 ms\n",
      "Wall time: 1min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pool.map(img_dwn_paral.download_images, jobs)\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7644c38",
   "metadata": {},
   "source": [
    "# Create vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c10e575",
   "metadata": {},
   "source": [
    "## 1) Create sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9480537",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = pd.read_parquet(os.path.join(conf['pth_dwn_samples'], \"samples.parquet\"), engine = \"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bcb97006",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples[\"sentence\"] = \\\n",
    "    np.where(samples[\"product_name\"], samples[\"product_name\"], \"\") + \" \" + \\\n",
    "    np.where(samples[\"collection_name\"], samples[\"collection_name\"], \"\") + \" \" + \\\n",
    "    np.where(samples[\"product_descr\"], samples[\"product_descr\"], \"\")\n",
    "\n",
    "samples[\"sentence\"] = samples[\"sentence\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9544f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = samples[\"sentence\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c294cdd",
   "metadata": {},
   "source": [
    "### 1.1) Fit tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f2f42041",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/home/ec2-user/SageMaker/foodi-ml/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "22c64136",
   "metadata": {},
   "outputs": [],
   "source": [
    "from retrieval.data.tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "30133f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(vocab_path=None, download_tokenizer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5e76701b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<pad>': 0, '<unk>': 1, '<start>': 2, '<end>': 3}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab.word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "01ac1214",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8011/8011 [00:01<00:00, 7386.66it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab = tokenizer.fit(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3be2438c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving vocabulary\n",
    "tokenizer.save(conf['pth_vocab'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f38a7e",
   "metadata": {},
   "source": [
    "### 1.2) Creating dataset json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d6a5d3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_special_characters(sentence):\n",
    "    # Clean isolated characters\n",
    "    remove_chars = ['.', '-', ')', '(','[', ']','{','}','?','!','€','$','#','@','*', '/']\n",
    "    for char in remove_chars:\n",
    "        if char in sentence:\n",
    "            sentence.remove(char)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "845faa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the folder to dump images\n",
    "img_dwn_paral = utils_aws.ImageDownloaderParallelS3(\n",
    "    base_path=conf['pth_dwn_images']\n",
    ")\n",
    "# Create iterable of jobs and modify img_path column\n",
    "jobs, samples = img_dwn_paral.create_jobs(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "308ddf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_dataset = {\n",
    "    \"images\": [],\n",
    "    \"dataset\": \"foodiml\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8ae137c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = samples.shape[0]\n",
    "samples_train = int(sample_size * 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "18f404c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples train:  5607\n",
      "Samples valid:  2404\n"
     ]
    }
   ],
   "source": [
    "print(\"Samples train: \", samples_train)\n",
    "print(\"Samples valid: \", sample_size - samples_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2bc7009b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8011it [00:02, 3946.59it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, row in tqdm.tqdm(samples.iterrows()):\n",
    "    raw_sentence = row[\"sentence\"]\n",
    "    filename = row[\"img_path\"].split(\"/\")[-1]\n",
    "    sentence_tokens = tokenizer.split_sentence(raw_sentence)\n",
    "    sentence_json = {}\n",
    "    sentence_json[\"imgid\"] = i\n",
    "    sentence_json[\"sentences\"] = [\n",
    "        {\n",
    "            \"tokens\": sentence_tokens,\n",
    "            \"raw\": raw_sentence,\n",
    "            \"imgid\": i\n",
    "\n",
    "        }\n",
    "    ]\n",
    "    split = \"train\"\n",
    "    if i > samples_train:\n",
    "        split = \"val\"\n",
    "    sentence_json[\"split\"] = split\n",
    "    sentence_json[\"filename\"] = filename\n",
    "    samples_dataset[\"images\"].append(sentence_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1206c8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset_foodiml.json \n",
    "with open(conf['pth_dataset_json'], \"w\") as f:\n",
    "    json.dump(samples_dataset, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

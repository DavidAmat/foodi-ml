{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c729eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import sys\n",
    "import json\n",
    "import tqdm\n",
    "os.chdir(\"/home/ec2-user/SageMaker/foodi-ml/notebooks/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4c53914",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d68f243",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils_aws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a171c724",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils_aws = reload(utils_aws)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d656bfb",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ddd7fcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {\n",
    "    \"S3_BUCKET\": 'test-bucket-glovocds',\n",
    "    \"S3K_imgs\": 'artifacts/002/',\n",
    "    \"S3_file_samples\": 'training_data.csv',\n",
    "    \"pth_dwn_samples\": '/home/ec2-user/SageMaker/data/samples',\n",
    "    \"pth_dwn_images\": '/home/ec2-user/SageMaker/data/images',\n",
    "    \"pth_vocab\": '/home/ec2-user/SageMaker/foodi-ml/.vocab_cache/foodiml_vocab.json',\n",
    "    \"pth_dataset_json\": '/home/ec2-user/SageMaker/data/foodiml/dataset_foodiml.json',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0f0782",
   "metadata": {},
   "source": [
    "# AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7534822d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS classes\n",
    "aws_con = utils_aws.AWSConnector(conf[\"S3_BUCKET\"])\n",
    "awstools = utils_aws.AWSTools(aws_con)\n",
    "aws_basics = utils_aws.AWSBasics(conf[\"S3_BUCKET\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9020e572",
   "metadata": {},
   "source": [
    "# List cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8941515b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BCN', 'CUG']\n"
     ]
    }
   ],
   "source": [
    "l_cities = awstools.create_list_cities(conf['S3K_imgs'])\n",
    "print(l_cities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024fd169",
   "metadata": {},
   "source": [
    "# Download samples CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adc045b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key artifacts/002/BCN/training_data.csv not found in S3\n",
      "Removing from l_cities city BCN\n",
      "City CUG correctly downloaded to /home/ec2-user/SageMaker/data/samples/CUG.csv\n"
     ]
    }
   ],
   "source": [
    "l_cities = awstools.downloading_city_csv(\n",
    "    l_cities=l_cities,\n",
    "    s3_key_prefix=conf['S3K_imgs'],\n",
    "    csv_name=conf['S3_file_samples'],\n",
    "    local_folder=conf['pth_dwn_samples'],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c1b705",
   "metadata": {},
   "source": [
    "# Read and concatenate CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8abd9250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all csv we downloaded\n",
    "l_csv = os.listdir(conf['pth_dwn_samples'])\n",
    "\n",
    "# Impose only .csv\n",
    "l_csv = [file_name for file_name in l_csv if file_name.endswith(\".csv\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1df9feb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenation to a single dataframe\n",
    "samples = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d1b2a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 13.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# Read and concatenate each city dataframe to samples\n",
    "for city_csv_file in tqdm.tqdm(l_csv):\n",
    "    path_csv = os.path.join(conf['pth_dwn_samples'], city_csv_file)\n",
    "    df_city_csv = pd.read_csv(path_csv)\n",
    "    df_city_csv.insert(loc=0, \n",
    "                       column=\"city\", \n",
    "                       value = city_csv_file.split(\".csv\")[0])\n",
    "    samples = pd.concat([samples, df_city_csv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53db09ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.to_parquet(os.path.join(conf['pth_dwn_samples'], \"samples.parquet\"), engine = \"pyarrow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863a7ad7",
   "metadata": {},
   "source": [
    "# Download images specified in samples CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a61395cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = pd.read_parquet(os.path.join(conf['pth_dwn_samples'], \"samples.parquet\"), engine = \"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "98d35372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the folder to dump images\n",
    "img_dwn_paral = utils_aws.ImageDownloaderParallelS3(\n",
    "    base_path=conf['pth_dwn_images']\n",
    ")\n",
    "# Create iterable of jobs and modify img_path column\n",
    "jobs, samples = img_dwn_paral.create_jobs(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff1cb6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a process pool to do the work\n",
    "pool = multiprocessing.Pool(\n",
    "    multiprocessing.cpu_count(), \n",
    "    img_dwn_paral.initialize,\n",
    "    (conf['S3_BUCKET'],)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2cdc00c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48.9 ms, sys: 18.9 ms, total: 67.8 ms\n",
      "Wall time: 1min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pool.map(img_dwn_paral.download_images, jobs)\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b71d8b",
   "metadata": {},
   "source": [
    "# Create vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64c3cb5",
   "metadata": {},
   "source": [
    "## 1) Create sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2655aea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = pd.read_parquet(os.path.join(conf['pth_dwn_samples'], \"samples.parquet\"), engine = \"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "934bcc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples[\"sentence\"] = \\\n",
    "    np.where(samples[\"product_name\"], samples[\"product_name\"], \"\") + \" \" + \\\n",
    "    np.where(samples[\"collection_name\"], samples[\"collection_name\"], \"\") + \" \" + \\\n",
    "    np.where(samples[\"product_descr\"], samples[\"product_descr\"], \"\")\n",
    "\n",
    "samples[\"sentence\"] = samples[\"sentence\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cd1645a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = samples[\"sentence\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ac8e75",
   "metadata": {},
   "source": [
    "### 1.1) Fit tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "036acab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/home/ec2-user/SageMaker/foodi-ml/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "34a8ac57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from retrieval.data.tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6fd72cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(vocab_path=None, download_tokenizer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "188f4486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<pad>': 0, '<unk>': 1, '<start>': 2, '<end>': 3}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab.word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1177956b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8011/8011 [00:01<00:00, 7386.66it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab = tokenizer.fit(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fa05fa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving vocabulary\n",
    "tokenizer.save(conf['pth_vocab'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad6e51d",
   "metadata": {},
   "source": [
    "### 1.2) Creating dataset json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bd9cbeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_special_characters(sentence):\n",
    "    # Clean isolated characters\n",
    "    remove_chars = ['.', '-', ')', '(','[', ']','{','}','?','!','€','$','#','@','*', '/']\n",
    "    for char in remove_chars:\n",
    "        if char in sentence:\n",
    "            sentence.remove(char)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1930a245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the folder to dump images\n",
    "img_dwn_paral = utils_aws.ImageDownloaderParallelS3(\n",
    "    base_path=conf['pth_dwn_images']\n",
    ")\n",
    "# Create iterable of jobs and modify img_path column\n",
    "jobs, samples = img_dwn_paral.create_jobs(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e3a6b43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_dataset = {\n",
    "    \"images\": [],\n",
    "    \"dataset\": \"foodiml\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c56bcd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = samples.shape[0]\n",
    "samples_train = int(sample_size * 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "52b359fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples train:  5607\n",
      "Samples valid:  2404\n"
     ]
    }
   ],
   "source": [
    "print(\"Samples train: \", samples_train)\n",
    "print(\"Samples valid: \", sample_size - samples_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "619023de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8011it [00:02, 3946.59it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, row in tqdm.tqdm(samples.iterrows()):\n",
    "    raw_sentence = row[\"sentence\"]\n",
    "    filename = row[\"img_path\"].split(\"/\")[-1]\n",
    "    sentence_tokens = tokenizer.split_sentence(raw_sentence)\n",
    "    sentence_json = {}\n",
    "    sentence_json[\"imgid\"] = i\n",
    "    sentence_json[\"sentences\"] = [\n",
    "        {\n",
    "            \"tokens\": sentence_tokens,\n",
    "            \"raw\": raw_sentence,\n",
    "            \"imgid\": i\n",
    "\n",
    "        }\n",
    "    ]\n",
    "    split = \"train\"\n",
    "    if i > samples_train:\n",
    "        split = \"val\"\n",
    "    sentence_json[\"split\"] = split\n",
    "    sentence_json[\"filename\"] = filename\n",
    "    samples_dataset[\"images\"].append(sentence_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0b907a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset_foodiml.json \n",
    "with open(conf['pth_dataset_json'], \"w\") as f:\n",
    "    json.dump(samples_dataset, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

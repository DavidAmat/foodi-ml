{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cba96a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import sys\n",
    "import json\n",
    "import tqdm\n",
    "os.chdir(\"/home/ec2-user/SageMaker/foodi-ml/notebooks/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba3ef6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "654a19c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils_aws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7e6626f",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils_aws = reload(utils_aws)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb0985a",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d159546",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {\n",
    "    \"S3_BUCKET\": 'test-bucket-glovocds',\n",
    "    \"S3K_imgs\": 'artifacts/002/',\n",
    "    \"S3_file_samples\": 'training_data.csv',\n",
    "    \"pth_dwn_samples\": '/home/ec2-user/SageMaker/data/samples',\n",
    "    \"pth_dwn_images\": '/home/ec2-user/SageMaker/data/images',\n",
    "    \"pth_vocab\": '/home/ec2-user/SageMaker/foodi-ml/.vocab_cache/foodiml_vocab.json',\n",
    "    \"pth_dataset_json\": '/home/ec2-user/SageMaker/data/foodiml/dataset_foodiml.json',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0df275",
   "metadata": {},
   "source": [
    "# AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58fecf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS classes\n",
    "aws_con = utils_aws.AWSConnector(conf[\"S3_BUCKET\"])\n",
    "awstools = utils_aws.AWSTools(aws_con)\n",
    "aws_basics = utils_aws.AWSBasics(conf[\"S3_BUCKET\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be4add6",
   "metadata": {},
   "source": [
    "# List cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5666160e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BCN', 'CUG']\n"
     ]
    }
   ],
   "source": [
    "l_cities = awstools.create_list_cities(conf['S3K_imgs'])\n",
    "print(l_cities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7d0e04",
   "metadata": {},
   "source": [
    "# Download samples CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3642b03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key artifacts/002/BCN/training_data.csv not found in S3\n",
      "Removing from l_cities city BCN\n",
      "City CUG correctly downloaded to /home/ec2-user/SageMaker/data/samples/CUG.csv\n"
     ]
    }
   ],
   "source": [
    "l_cities = awstools.downloading_city_csv(\n",
    "    l_cities=l_cities,\n",
    "    s3_key_prefix=conf['S3K_imgs'],\n",
    "    csv_name=conf['S3_file_samples'],\n",
    "    local_folder=conf['pth_dwn_samples'],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3eb775",
   "metadata": {},
   "source": [
    "# Read and concatenate CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c22b847f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all csv we downloaded\n",
    "l_csv = os.listdir(conf['pth_dwn_samples'])\n",
    "\n",
    "# Impose only .csv\n",
    "l_csv = [file_name for file_name in l_csv if file_name.endswith(\".csv\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cb9c156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenation to a single dataframe\n",
    "samples = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47a0a226",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 14.17it/s]\n"
     ]
    }
   ],
   "source": [
    "# Read and concatenate each city dataframe to samples\n",
    "for city_csv_file in tqdm.tqdm(l_csv):\n",
    "    path_csv = os.path.join(conf['pth_dwn_samples'], city_csv_file)\n",
    "    df_city_csv = pd.read_csv(path_csv)\n",
    "    df_city_csv.insert(loc=0, \n",
    "                       column=\"city\", \n",
    "                       value = city_csv_file.split(\".csv\")[0])\n",
    "    samples = pd.concat([samples, df_city_csv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddbfff19",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.to_parquet(os.path.join(conf['pth_dwn_samples'], \"samples.parquet\"), engine = \"pyarrow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6dc65a",
   "metadata": {},
   "source": [
    "# Download images specified in samples CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96ddc7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = pd.read_parquet(os.path.join(conf['pth_dwn_samples'], \"samples.parquet\"), engine = \"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6aab4cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the folder to dump images\n",
    "img_dwn_paral = utils_aws.ImageDownloaderParallelS3(\n",
    "    base_path=conf['pth_dwn_images']\n",
    ")\n",
    "# Create iterable of jobs and modify img_path column\n",
    "jobs, samples = img_dwn_paral.create_jobs(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "654fc3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a process pool to do the work\n",
    "pool = multiprocessing.Pool(\n",
    "    multiprocessing.cpu_count(), \n",
    "    img_dwn_paral.initialize,\n",
    "    (conf['S3_BUCKET'],)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8bff6257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48.9 ms, sys: 18.9 ms, total: 67.8 ms\n",
      "Wall time: 1min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pool.map(img_dwn_paral.download_images, jobs)\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3b37fd",
   "metadata": {},
   "source": [
    "# Create vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208dbfe3",
   "metadata": {},
   "source": [
    "## 1) Create sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2be74dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = pd.read_parquet(os.path.join(conf['pth_dwn_samples'], \"samples.parquet\"), engine = \"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a0214ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples[\"sentence\"] = \\\n",
    "    np.where(samples[\"product_name\"], samples[\"product_name\"], \"\") + \" \" + \\\n",
    "    np.where(samples[\"collection_name\"], samples[\"collection_name\"], \"\") + \" \" + \\\n",
    "    np.where(samples[\"product_descr\"], samples[\"product_descr\"], \"\")\n",
    "\n",
    "samples[\"sentence\"] = samples[\"sentence\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29f2554a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = samples[\"sentence\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414d9445",
   "metadata": {},
   "source": [
    "### 1.1) Fit tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5e3c0c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/home/ec2-user/SageMaker/foodi-ml/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c1916017",
   "metadata": {},
   "outputs": [],
   "source": [
    "from retrieval.data.tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "80f6deba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(vocab_path=None, download_tokenizer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ef0fe879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<pad>': 0, '<unk>': 1, '<start>': 2, '<end>': 3}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab.word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f7d4ce11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8011/8011 [00:01<00:00, 7386.66it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab = tokenizer.fit(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9681eef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving vocabulary\n",
    "tokenizer.save(conf['pth_vocab'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a78a2dc",
   "metadata": {},
   "source": [
    "### 1.2) Creating dataset json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bb944cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_special_characters(sentence):\n",
    "    # Clean isolated characters\n",
    "    remove_chars = ['.', '-', ')', '(','[', ']','{','}','?','!','€','$','#','@','*', '/']\n",
    "    for char in remove_chars:\n",
    "        if char in sentence:\n",
    "            sentence.remove(char)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bb870526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the folder to dump images\n",
    "img_dwn_paral = utils_aws.ImageDownloaderParallelS3(\n",
    "    base_path=conf['pth_dwn_images']\n",
    ")\n",
    "# Create iterable of jobs and modify img_path column\n",
    "jobs, samples = img_dwn_paral.create_jobs(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "670a131e",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_dataset = {\n",
    "    \"images\": [],\n",
    "    \"dataset\": \"foodiml\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "281e480c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = samples.shape[0]\n",
    "samples_train = int(sample_size * 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2c19ac8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples train:  5607\n",
      "Samples valid:  2404\n"
     ]
    }
   ],
   "source": [
    "print(\"Samples train: \", samples_train)\n",
    "print(\"Samples valid: \", sample_size - samples_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "545f642a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8011it [00:02, 3946.59it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, row in tqdm.tqdm(samples.iterrows()):\n",
    "    raw_sentence = row[\"sentence\"]\n",
    "    filename = row[\"img_path\"].split(\"/\")[-1]\n",
    "    sentence_tokens = tokenizer.split_sentence(raw_sentence)\n",
    "    sentence_json = {}\n",
    "    sentence_json[\"imgid\"] = i\n",
    "    sentence_json[\"sentences\"] = [\n",
    "        {\n",
    "            \"tokens\": sentence_tokens,\n",
    "            \"raw\": raw_sentence,\n",
    "            \"imgid\": i\n",
    "\n",
    "        }\n",
    "    ]\n",
    "    split = \"train\"\n",
    "    if i > samples_train:\n",
    "        split = \"val\"\n",
    "    sentence_json[\"split\"] = split\n",
    "    sentence_json[\"filename\"] = filename\n",
    "    samples_dataset[\"images\"].append(sentence_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "98794e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset_foodiml.json \n",
    "with open(conf['pth_dataset_json'], \"w\") as f:\n",
    "    json.dump(samples_dataset, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

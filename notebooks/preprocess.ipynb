{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb1ad208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import sys\n",
    "import json\n",
    "import tqdm\n",
    "os.chdir(\"/home/ec2-user/SageMaker/foodi-ml/notebooks/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f96e9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63b0ca06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils_aws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "794a2913",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils_aws = reload(utils_aws)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d27930",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "92a9f24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {\n",
    "    \"S3_BUCKET\": 'test-bucket-glovocds',\n",
    "    \"S3K_imgs\": 'artifacts/002/',\n",
    "    \"S3_file_samples\": 'training_data.csv',\n",
    "    \"pth_dwn_samples\": '/home/ec2-user/SageMaker/data/samples',\n",
    "    \"pth_dwn_images\": '/home/ec2-user/SageMaker/data/images',\n",
    "    \"pth_vocab\": '/home/ec2-user/SageMaker/foodi-ml/.vocab_cache/foodiml_vocab.json',\n",
    "    \"pth_dataset_json\": '/home/ec2-user/SageMaker/data/dataset_foodiml.json',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8584c1af",
   "metadata": {},
   "source": [
    "# AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5cfd65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS classes\n",
    "aws_con = utils_aws.AWSConnector(conf[\"S3_BUCKET\"])\n",
    "awstools = utils_aws.AWSTools(aws_con)\n",
    "aws_basics = utils_aws.AWSBasics(conf[\"S3_BUCKET\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61a54cd",
   "metadata": {},
   "source": [
    "# List cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23b49190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BCN', 'CUG']\n"
     ]
    }
   ],
   "source": [
    "l_cities = awstools.create_list_cities(conf['S3K_imgs'])\n",
    "print(l_cities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f87537f",
   "metadata": {},
   "source": [
    "# Download samples CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4702a153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key artifacts/002/BCN/training_data.csv not found in S3\n",
      "Removing from l_cities city BCN\n",
      "City CUG correctly downloaded to /home/ec2-user/SageMaker/data/samples/CUG.csv\n"
     ]
    }
   ],
   "source": [
    "l_cities = awstools.downloading_city_csv(\n",
    "    l_cities=l_cities,\n",
    "    s3_key_prefix=conf['S3K_imgs'],\n",
    "    csv_name=conf['S3_file_samples'],\n",
    "    local_folder=conf['pth_dwn_samples'],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874bc0e4",
   "metadata": {},
   "source": [
    "# Read and concatenate CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6628f9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all csv we downloaded\n",
    "l_csv = os.listdir(conf['pth_dwn_samples'])\n",
    "\n",
    "# Impose only .csv\n",
    "l_csv = [file_name for file_name in l_csv if file_name.endswith(\".csv\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0663f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenation to a single dataframe\n",
    "samples = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e03262af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 23.62it/s]\n"
     ]
    }
   ],
   "source": [
    "# Read and concatenate each city dataframe to samples\n",
    "for city_csv_file in tqdm.tqdm(l_csv):\n",
    "    path_csv = os.path.join(conf['pth_dwn_samples'], city_csv_file)\n",
    "    df_city_csv = pd.read_csv(path_csv)\n",
    "    df_city_csv.insert(loc=0, \n",
    "                       column=\"city\", \n",
    "                       value = city_csv_file.split(\".csv\")[0])\n",
    "    samples = pd.concat([samples, df_city_csv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11ec5d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.to_parquet(os.path.join(conf['pth_dwn_samples'], \"samples.parquet\"), engine = \"pyarrow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badf5c91",
   "metadata": {},
   "source": [
    "# Download images specified in samples CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9fa7f4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = pd.read_parquet(os.path.join(conf['pth_dwn_samples'], \"samples.parquet\"), engine = \"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0c3d0df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the folder to dump images\n",
    "img_dwn_paral = utils_aws.ImageDownloaderParallelS3(\n",
    "    base_path=conf['pth_dwn_images']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f21e21c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create iterable of jobs and modify img_path column\n",
    "jobs, samples = img_dwn_paral.create_jobs(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "982532af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a process pool to do the work\n",
    "pool = multiprocessing.Pool(\n",
    "    multiprocessing.cpu_count(), \n",
    "    img_dwn_paral.initialize,\n",
    "    (conf['S3_BUCKET'],)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd6bd19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48.9 ms, sys: 18.9 ms, total: 67.8 ms\n",
      "Wall time: 1min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pool.map(img_dwn_paral.download_images, jobs)\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9463a6",
   "metadata": {},
   "source": [
    "# Create vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be97cf3",
   "metadata": {},
   "source": [
    "## Create sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "64cd74a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples[\"sentence\"] = \\\n",
    "    np.where(samples[\"product_name\"], samples[\"product_name\"], \"\") + \" \" + \\\n",
    "    np.where(samples[\"collection_name\"], samples[\"collection_name\"], \"\") + \" \" + \\\n",
    "    np.where(samples[\"product_descr\"], samples[\"product_descr\"], \"\")\n",
    "\n",
    "samples[\"sentence\"] = samples[\"sentence\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "715467ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = samples[\"sentence\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464a4522",
   "metadata": {},
   "source": [
    "## Fit tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a2223eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/home/ec2-user/SageMaker/foodi-ml/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c72e2c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from retrieval.data.tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74f2bd6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(vocab_path=None, download_tokenizer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7cd6ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<pad>': 0, '<unk>': 1, '<start>': 2, '<end>': 3}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab.word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2978b8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8011/8011 [00:01<00:00, 7161.87it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab = tokenizer.fit(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b5ad399c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving vocabulary\n",
    "tokenizer.save(conf['pth_vocab'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20213b92",
   "metadata": {},
   "source": [
    "# Creating dataset json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6618a99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_special_characters(sentence):\n",
    "    # Clean isolated characters\n",
    "    remove_chars = ['.', '-', ')', '(','[', ']','{','}','?','!','€','$','#','@','*', '/']\n",
    "    for char in remove_chars:\n",
    "        if char in sentence:\n",
    "            sentence.remove(char)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "63c2a9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_dataset = {\n",
    "    \"images\": [],\n",
    "    \"dataset\": \"foodiml\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "25895873",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8011it [00:02, 3724.56it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, row in tqdm.tqdm(samples.iterrows()):\n",
    "    raw_sentence = row[\"sentence\"]\n",
    "    filename = row[\"img_path\"].split(\"/\")[-1]\n",
    "    sentence_tokens = tokenizer.split_sentence(raw_sentence)\n",
    "    sentence_json = {}\n",
    "    sentence_json[\"imgid\"] = i\n",
    "    sentence_json[\"sentences\"] = [\n",
    "        {\n",
    "            \"tokens\": sentence_tokens,\n",
    "            \"raw\": raw_sentence,\n",
    "            \"imgid\": i\n",
    "\n",
    "        }\n",
    "    ]\n",
    "    sentence_json[\"split\"] = \"train\"\n",
    "    sentence_json[\"filename\"] = filename\n",
    "    samples_dataset[\"images\"].append(sentence_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "80158cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset_foodiml.json \n",
    "with open(conf['pth_dataset_json'], \"w\") as f:\n",
    "    json.dump(samples_dataset, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f50a0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import sys\n",
    "import json\n",
    "import tqdm\n",
    "os.chdir(\"/home/ec2-user/SageMaker/foodi-ml/notebooks/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cdf35fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee9c76c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils_aws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20352bf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils_aws' from '/home/ec2-user/SageMaker/foodi-ml/notebooks/utils_aws.py'>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils_aws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48ec62d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils_aws = reload(utils_aws)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfddb7ab",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1747bb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {\n",
    "    \"S3_BUCKET\": 'test-bucket-glovocds',\n",
    "    \"S3K_imgs\": 'artifacts/002/',\n",
    "    \"S3_file_samples\": 'training_data.csv',\n",
    "    \"pth_dwn_samples\": '/home/ec2-user/SageMaker/data/samples',\n",
    "    \"pth_dwn_images\": '/home/ec2-user/SageMaker/data/images',\n",
    "    \"pth_vocab\": '/home/ec2-user/SageMaker/foodi-ml/.vocab_cache/foodiml_vocab.json',\n",
    "    \"pth_dataset_json\": '/home/ec2-user/SageMaker/data/foodiml/dataset_foodiml.json',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7c8595",
   "metadata": {},
   "source": [
    "# AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ae32732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS classes\n",
    "aws_con = utils_aws.AWSConnector(conf[\"S3_BUCKET\"])\n",
    "awstools = utils_aws.AWSTools(aws_con)\n",
    "aws_basics = utils_aws.AWSBasics(conf[\"S3_BUCKET\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5ce98d",
   "metadata": {},
   "source": [
    "# List cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0af169f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BCN', 'CUG']\n"
     ]
    }
   ],
   "source": [
    "l_cities = awstools.create_list_cities(conf['S3K_imgs'])\n",
    "print(l_cities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a9f2c7",
   "metadata": {},
   "source": [
    "# Download samples CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8fa52cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_cities = ['CUG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91bb1452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key artifacts/002/CUG/training_data.csv not found in S3\n",
      "Removing from l_cities city CUG\n"
     ]
    }
   ],
   "source": [
    "l_cities = awstools.downloading_city_csv(\n",
    "    l_cities=l_cities,\n",
    "    s3_key_prefix=conf['S3K_imgs'],\n",
    "    csv_name=conf['S3_file_samples'],\n",
    "    local_folder=conf['pth_dwn_samples'],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15cf92ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'training_data.csv'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf['S3_file_samples']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98993574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ec2-user/SageMaker/data/samples'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf['pth_dwn_samples']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68180cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "city = \"CUG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "213780e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key artifacts/002/CUG/training_data.csv not found in S3\n"
     ]
    }
   ],
   "source": [
    "# Get the prefix of the S3 csv name\n",
    "local_file_name = f\"{city}.csv\"\n",
    "\n",
    "# Get full local path\n",
    "local_file_path = os.path.join(conf['pth_dwn_samples'], local_file_name)\n",
    "\n",
    "# S3 key of the .csv\n",
    "s3k_file_path = os.path.join(conf['S3K_imgs'], city, conf['S3_file_samples'])\n",
    "\n",
    "# Download csv\n",
    "success = aws_basics.download_obj(\n",
    "    s3_key=s3k_file_path,\n",
    "    destination=local_file_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0fd6055d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "success"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409361bc",
   "metadata": {},
   "source": [
    "# Read and concatenate CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a9bf426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all csv we downloaded\n",
    "l_csv = os.listdir(conf['pth_dwn_samples'])\n",
    "\n",
    "# Impose only .csv\n",
    "l_csv = [file_name for file_name in l_csv if file_name.endswith(\".csv\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "353a48b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ec2-user/SageMaker/data/samples'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf['pth_dwn_samples']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc4cee4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenation to a single dataframe\n",
    "samples = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4546548f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# Read and concatenate each city dataframe to samples\n",
    "for city_csv_file in tqdm.tqdm(l_csv):\n",
    "    path_csv = os.path.join(conf['pth_dwn_samples'], city_csv_file)\n",
    "    df_city_csv = pd.read_csv(path_csv)\n",
    "    df_city_csv.insert(loc=0,\n",
    "                       column=\"city\", \n",
    "                       value = city_csv_file.split(\".csv\")[0])\n",
    "    samples = pd.concat([samples, df_city_csv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "772c0307",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NEW UPDATES PONÃ‡ FINAL REVIEWS\n",
    "# Now we have the entire dataframe, we read it and dump it into a parquet in the same way david was doing\n",
    "import pandas as pd\n",
    "import os\n",
    "conf = {\n",
    "    \"S3_BUCKET\": 'test-bucket-glovocds',\n",
    "    \"S3K_imgs\": 'artifacts/002/',\n",
    "    \"S3_file_samples\": 'glovo-foodi-ml-dataset.csv',\n",
    "    \"pth_dwn_samples\": '/home/ec2-user/SageMaker/dataset/',\n",
    "    \"pth_dwn_images\": '/home/ec2-user/SageMaker/dataset/dataset/',\n",
    "    \"pth_vocab\": '/home/ec2-user/SageMaker/foodi-ml/.vocab_cache/foodiml_vocab.json',\n",
    "    \"pth_dataset_json\": '/home/ec2-user/SageMaker/data/foodiml/dataset_foodiml.json',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6defb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = pd.read_csv(os.path.join(conf['pth_dwn_samples'], conf[\"S3_file_samples\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d9fc91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.to_parquet(os.path.join(conf['pth_dwn_samples'], \"samples.parquet\"), engine = \"pyarrow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450d2b2d",
   "metadata": {},
   "source": [
    "# Download images specified in samples CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb4e8655",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = pd.read_parquet(os.path.join(conf['pth_dwn_samples'], \"samples.parquet\"), engine = \"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fa1b3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ab3e07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db1adbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab6e557f",
   "metadata": {},
   "source": [
    "# Create vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cd0725",
   "metadata": {},
   "source": [
    "## 1) Create sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a14604e",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = pd.read_parquet(os.path.join(conf['pth_dwn_samples'], \"samples.parquet\"), engine = \"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c79607c",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples[\"sentence\"] = \\\n",
    "    np.where(samples[\"product_name\"], samples[\"product_name\"], \"\") + \" \" + \\\n",
    "    np.where(samples[\"collection_section\"], samples[\"collection_section\"], \"\") + \" \" + \\\n",
    "    np.where(samples[\"product_description\"], samples[\"product_description\"], \"\")\n",
    "\n",
    "samples[\"sentence\"] = samples[\"sentence\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1595ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = samples[\"sentence\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afcfb31",
   "metadata": {},
   "source": [
    "### 1.1)Â Fit tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cf65dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/home/ec2-user/SageMaker/foodi-ml/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9df17199",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-b50fa9d20c6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mretrieval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/SageMaker/foodi-ml/retrieval/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SageMaker/foodi-ml/retrieval/model/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRetrieval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msimilarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimgenc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SageMaker/foodi-ml/retrieval/model/model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_parallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_logger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "from retrieval.data.tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "acea9923",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(vocab_path=None, download_tokenizer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ae4e9370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<pad>': 0, '<unk>': 1, '<start>': 2, '<end>': 3}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab.word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0f7b21ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8011/8011 [00:01<00:00, 7386.66it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab = tokenizer.fit(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a186ba9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â Saving vocabulary\n",
    "tokenizer.save(conf['pth_vocab'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baabc6a2",
   "metadata": {},
   "source": [
    "### 1.2) Creating dataset json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cbeffe19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_special_characters(sentence):\n",
    "    # Clean isolated characters\n",
    "    remove_chars = ['.', '-', ')', '(','[', ']','{','}','?','!','â‚¬','$','#','@','*', '/']\n",
    "    for char in remove_chars:\n",
    "        if char in sentence:\n",
    "            sentence.remove(char)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5ec1e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2bcca90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_dataset = {\n",
    "    \"images\": [],\n",
    "    \"dataset\": \"foodiml\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cd8fd721",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = samples.shape[0]\n",
    "samples_train = int(sample_size * 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f59f510c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples train:  5607\n",
      "Samples valid:  2404\n"
     ]
    }
   ],
   "source": [
    "print(\"Samples train: \", samples_train)\n",
    "print(\"Samples valid: \", sample_size - samples_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31b4acb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Unnamed: 0.1', 'country_code', 'city_code', 'store_name',\n",
       "       'product_name', 'collection_section', 'product_description', 'subset',\n",
       "       'hash', 'aux_store', 'HIER', 's3_path', 'sentence'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6758b76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train    2021210\n",
       "test      433117\n",
       "val       433117\n",
       "Name: subset, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples.subset.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "55f82686",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8011it [00:02, 3946.59it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, row in tdm(samples.iterrows()):\n",
    "    raw_sentence = row[\"sentence\"]\n",
    "    filename = row[\"s3_path\"].split(\"/\")[-1]\n",
    "    sentence_tokens = tokenizer.split_sentence(raw_sentence)\n",
    "    sentence_json = {}\n",
    "    sentence_json[\"imgid\"] = i\n",
    "    sentence_json[\"sentences\"] = [\n",
    "        {\n",
    "            \"tokens\": sentence_tokens,\n",
    "            \"raw\": raw_sentence,\n",
    "            \"imgid\": i\n",
    "        }\n",
    "    ]\n",
    "    sentence_json[\"split\"] = row[\"subset\"]\n",
    "    sentence_json[\"filename\"] = filename\n",
    "    samples_dataset[\"images\"].append(sentence_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9d27876f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset_foodiml.json \n",
    "with open(conf['pth_dataset_json'], \"w\") as f:\n",
    "    json.dump(samples_dataset, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96a3c21e",
   "metadata": {},
   "source": [
    "# DEEP DIVE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f45f2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/ec2-user/SageMaker/foodi-ml/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dfed12b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88bd8811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "import params\n",
    "from retrieval.train import train\n",
    "from retrieval.utils import helper\n",
    "from retrieval.model import loss\n",
    "from retrieval.model.model import Retrieval\n",
    "from retrieval.data.loaders import get_loaders\n",
    "from retrieval.utils.logger import create_logger\n",
    "from retrieval.utils.helper import load_model\n",
    "from retrieval.utils.file_utils import load_yaml_opts, parse_loader_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ad04866",
   "metadata": {},
   "outputs": [],
   "source": [
    "from addict import Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d88c94c",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e40f269e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_path(opt):\n",
    "    if 'DATA_PATH' not in os.environ:\n",
    "        if not opt.dataset.data_path:\n",
    "            raise Exception('''\n",
    "                DATA_PATH not specified.\n",
    "                Please, run \"$ export DATA_PATH=/path/to/dataset\"\n",
    "                or add path to yaml file\n",
    "            ''')\n",
    "        return opt.dataset.data_path\n",
    "    else:\n",
    "        return os.environ['DATA_PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5118ea9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizers(train_loader):\n",
    "    tokenizers = train_loader.dataset.tokenizer\n",
    "    if type(tokenizers) != list:\n",
    "        tokenizers = [tokenizers]\n",
    "    return tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7e3abd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_criterion(opt, model):\n",
    "    if 'name' in opt.criterion:\n",
    "        logger.info(opt.criterion)\n",
    "        multimodal_criterion = loss.get_loss(**opt.criterion)\n",
    "        multilanguage_criterion = loss.get_loss(**opt.criterion)\n",
    "    else:\n",
    "        multimodal_criterion = loss.ContrastiveLoss(**opt.criterion)\n",
    "        multilanguage_criterion = loss.ContrastiveLoss(**opt.ml_criterion)\n",
    "    set_model_criterion(opt, model, multilanguage_criterion, multimodal_criterion)\n",
    "    # return multimodal_criterion, multilanguage_criterion\n",
    "\n",
    "\n",
    "def set_model_criterion(opt, model, multilanguage_criterion, multimodal_criterion):\n",
    "    model.mm_criterion = multimodal_criterion\n",
    "    model.ml_criterion = None\n",
    "    if len(opt.dataset.adapt.data) > 0:\n",
    "        model.ml_criterion = multilanguage_criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4b4d21",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47cb8914",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"DATA_PATH\"] = \"/home/ec2-user/SageMaker/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "234db904",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = \"options/adapt/foodi-ml/i2t.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5857df99",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"options\": options,\n",
    "}\n",
    "args = Dict(args)\n",
    "opt = load_yaml_opts(args.options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01a3b97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = create_logger(level='debug' if opt.engine.debug else 'info')\n",
    "#logger.info(f'Used args   : \\n{args}')\n",
    "#logger.info(f'Used options: \\n{opt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f62e20b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get path of the data\n",
    "data_path = get_data_path(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1df5b1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-16 18:32:12,175 - [INFO    ] - Loaded vocab containing 2487 tokens\n",
      "2021-08-16 18:32:12,176 - [INFO    ] - Loaded from .vocab_cache/foodiml_vocab.json.\n",
      "2021-08-16 18:32:12,177 - [INFO    ] - Created tokenizer with init 2487 tokens.\n",
      "2021-08-16 18:32:12,222 - [INFO    ] - [FoodiML] Loaded 8011 images and 8011 annotations.\n",
      "2021-08-16 18:32:12,227 - [INFO    ] - Loaded vocab containing 2487 tokens\n",
      "2021-08-16 18:32:12,228 - [INFO    ] - Loaded from .vocab_cache/foodiml_vocab.json.\n",
      "2021-08-16 18:32:12,228 - [INFO    ] - Created tokenizer with init 2487 tokens.\n",
      "2021-08-16 18:32:12,266 - [INFO    ] - [FoodiML] Loaded 0 images and 0 annotations.\n",
      "2021-08-16 18:32:12,267 - [INFO    ] - Adapt loaders: 0\n"
     ]
    }
   ],
   "source": [
    "# Get loaders\n",
    "train_loader, val_loaders, adapt_loaders = get_loaders(data_path, args.local_rank, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01ec88df",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers = get_tokenizers(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "caf4c8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-16 18:32:12,919 - [INFO    ] - Image encoder created: ('resnet50',)\n",
      "2021-08-16 18:32:13,027 - [INFO    ] - Text encoder created: gru_glove\n",
      "2021-08-16 18:32:13,028 - [INFO    ] - Created similarity: Cosine()\n",
      "2021-08-16 18:32:15,993 - [INFO    ] - Setting devices: img: cuda,txt: cuda, loss: cuda\n",
      "2021-08-16 18:32:15,994 - [INFO    ] - Using similarity: ('cosine',)\n"
     ]
    }
   ],
   "source": [
    "model = Retrieval(**opt.model, tokenizers=tokenizers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c41f91",
   "metadata": {},
   "source": [
    "<font color='red'> **Deep dive on Retrieval --------------------- (1)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b87b6ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from retrieval.model.model import Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87898ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.model['img_enc']['name'] = 'resnet50'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "caec5f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-16 18:32:16,631 - [INFO    ] - Image encoder created: ('resnet50',)\n",
      "2021-08-16 18:32:16,737 - [INFO    ] - Text encoder created: gru_glove\n",
      "2021-08-16 18:32:16,738 - [INFO    ] - Created similarity: Cosine()\n",
      "2021-08-16 18:32:16,792 - [INFO    ] - Setting devices: img: cuda,txt: cuda, loss: cuda\n",
      "2021-08-16 18:32:16,792 - [INFO    ] - Using similarity: ('cosine',)\n"
     ]
    }
   ],
   "source": [
    "model = Retrieval(**opt.model, tokenizers=tokenizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b4c3643",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_enc = model.txt_enc\n",
    "img_enc = model.img_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01f06341",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_pool = model.txt_pool \n",
    "img_pool = model.img_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "657a3bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_loader = iter(train_loader)\n",
    "batch = next(gen_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4e381d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(gen_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399e3afa",
   "metadata": {},
   "source": [
    "#### Images embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d417a7ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(batch) == dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69529ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch = batch['image']\n",
    "input_batch = input_batch.to(model.img_enc.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65477ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e0fda633",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tensor = img_enc(input_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e8ec9627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 224, 224])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8147e51e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 49, 1024])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "25b7c4b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 49, 1024])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "485c152e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_embed  = model.embed_image_features(img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "274ac6df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 49, 1024])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_embed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172135b3",
   "metadata": {},
   "source": [
    "#### Text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c4a26017",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "71bbd674",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_tensor, lengths = txt_enc(input_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "60778ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 25, 1024])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "035d2529",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_tensor = model.embed_caption_features(txt_tensor, lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "52fe091d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 25, 1024])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "12c0f40d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 8, 13, 12, 19, 20, 25, 11, 24, 8]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5a2e19e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 25])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['caption'][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf1947c",
   "metadata": {},
   "source": [
    "#### Forward batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9c1f07ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_embed, txt_embed = model.forward_batch(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6c03c6db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 49, 1024])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "171fd5b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 25, 1024])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "144f73ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.get_sim_matrix(img_embed, txt_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1187ae03",
   "metadata": {},
   "source": [
    "<font color='red'> **Finish Deep dive on Retrieval --------------------- (1)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

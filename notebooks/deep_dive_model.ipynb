{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46d1cf9d",
   "metadata": {},
   "source": [
    "# DEEP DIVE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1df4db07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/ec2-user/SageMaker/foodi-ml/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf89745b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54bd7880",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8efc2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "import params\n",
    "from retrieval.train import train\n",
    "from retrieval.utils import helper\n",
    "from retrieval.model import loss\n",
    "from retrieval.model.model import Retrieval\n",
    "from retrieval.data.loaders import get_loaders\n",
    "from retrieval.utils.logger import create_logger\n",
    "from retrieval.utils.helper import load_model\n",
    "from retrieval.utils.file_utils import load_yaml_opts, parse_loader_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9e429f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from addict import Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6159e309",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d9840ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_path(opt):\n",
    "    if 'DATA_PATH' not in os.environ:\n",
    "        if not opt.dataset.data_path:\n",
    "            raise Exception('''\n",
    "                DATA_PATH not specified.\n",
    "                Please, run \"$ export DATA_PATH=/path/to/dataset\"\n",
    "                or add path to yaml file\n",
    "            ''')\n",
    "        return opt.dataset.data_path\n",
    "    else:\n",
    "        return os.environ['DATA_PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d854151b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizers(train_loader):\n",
    "    tokenizers = train_loader.dataset.tokenizer\n",
    "    if type(tokenizers) != list:\n",
    "        tokenizers = [tokenizers]\n",
    "    return tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48aac5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_criterion(opt, model):\n",
    "    if 'name' in opt.criterion:\n",
    "        logger.info(opt.criterion)\n",
    "        multimodal_criterion = loss.get_loss(**opt.criterion)\n",
    "        multilanguage_criterion = loss.get_loss(**opt.criterion)\n",
    "    else:\n",
    "        multimodal_criterion = loss.ContrastiveLoss(**opt.criterion)\n",
    "        multilanguage_criterion = loss.ContrastiveLoss(**opt.ml_criterion)\n",
    "    set_model_criterion(opt, model, multilanguage_criterion, multimodal_criterion)\n",
    "    # return multimodal_criterion, multilanguage_criterion\n",
    "\n",
    "\n",
    "def set_model_criterion(opt, model, multilanguage_criterion, multimodal_criterion):\n",
    "    model.mm_criterion = multimodal_criterion\n",
    "    model.ml_criterion = None\n",
    "    if len(opt.dataset.adapt.data) > 0:\n",
    "        model.ml_criterion = multilanguage_criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c164352",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bfa80c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"DATA_PATH\"] = \"/home/ec2-user/SageMaker/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ccc570e",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = \"options/adapt/foodi-ml/i2t.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d51fb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"options\": options,\n",
    "}\n",
    "args = Dict(args)\n",
    "opt = load_yaml_opts(args.options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8042ec3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = create_logger(level='debug' if opt.engine.debug else 'info')\n",
    "#logger.info(f'Used args   : \\n{args}')\n",
    "#logger.info(f'Used options: \\n{opt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab2514df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get path of the data\n",
    "data_path = get_data_path(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8487ae69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-17 11:37:16,910 - [INFO    ] - Loaded vocab containing 2487 tokens\n",
      "2021-08-17 11:37:16,911 - [INFO    ] - Loaded from .vocab_cache/foodiml_vocab.json.\n",
      "2021-08-17 11:37:16,911 - [INFO    ] - Created tokenizer with init 2487 tokens.\n",
      "2021-08-17 11:37:16,955 - [INFO    ] - [FoodiML] Loaded 8011 images and 8011 annotations.\n",
      "2021-08-17 11:37:16,960 - [INFO    ] - Loaded vocab containing 2487 tokens\n",
      "2021-08-17 11:37:16,960 - [INFO    ] - Loaded from .vocab_cache/foodiml_vocab.json.\n",
      "2021-08-17 11:37:16,960 - [INFO    ] - Created tokenizer with init 2487 tokens.\n",
      "2021-08-17 11:37:16,997 - [INFO    ] - [FoodiML] Loaded 0 images and 0 annotations.\n",
      "2021-08-17 11:37:16,998 - [INFO    ] - Adapt loaders: 0\n"
     ]
    }
   ],
   "source": [
    "# Get loaders\n",
    "train_loader, val_loaders, adapt_loaders = get_loaders(data_path, args.local_rank, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "343b7a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers = get_tokenizers(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63f19141",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-17 11:37:18,512 - [INFO    ] - Image encoder created: ('full_image',)\n",
      "2021-08-17 11:37:18,804 - [INFO    ] - Text encoder created: gru_glove\n",
      "2021-08-17 11:37:18,874 - [INFO    ] - Created similarity: AdaptiveEmbeddingI2T(\n",
      "  (norm): Normalization(\n",
      "    (norm): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  )\n",
      "  (adapt_txt): ADAPT(\n",
      "    (fc_gamma): Sequential(\n",
      "      (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    )\n",
      "    (fc_beta): Sequential(\n",
      "      (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (fovea): Fovea(smooth=10,train_smooth: False)\n",
      ")\n",
      "2021-08-17 11:37:21,803 - [INFO    ] - Setting devices: img: cuda,txt: cuda, loss: cuda\n",
      "2021-08-17 11:37:21,804 - [INFO    ] - Using similarity: ('adapt_i2t',)\n"
     ]
    }
   ],
   "source": [
    "model = Retrieval(**opt.model, tokenizers=tokenizers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b5f3a2",
   "metadata": {},
   "source": [
    "<font color='red'> **Deep dive on Retrieval --------------------- (1)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a29bb377",
   "metadata": {},
   "outputs": [],
   "source": [
    "from retrieval.model.model import Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6decf92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'full_image'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.model['img_enc']['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c088a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-17 11:37:23,273 - [INFO    ] - Image encoder created: ('full_image',)\n",
      "2021-08-17 11:37:23,563 - [INFO    ] - Text encoder created: gru_glove\n",
      "2021-08-17 11:37:23,633 - [INFO    ] - Created similarity: AdaptiveEmbeddingI2T(\n",
      "  (norm): Normalization(\n",
      "    (norm): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  )\n",
      "  (adapt_txt): ADAPT(\n",
      "    (fc_gamma): Sequential(\n",
      "      (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    )\n",
      "    (fc_beta): Sequential(\n",
      "      (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (fovea): Fovea(smooth=10,train_smooth: False)\n",
      ")\n",
      "2021-08-17 11:37:23,779 - [INFO    ] - Setting devices: img: cuda,txt: cuda, loss: cuda\n",
      "2021-08-17 11:37:23,780 - [INFO    ] - Using similarity: ('adapt_i2t',)\n"
     ]
    }
   ],
   "source": [
    "model = Retrieval(**opt.model, tokenizers=tokenizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "841120d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_enc = model.txt_enc\n",
    "img_enc = model.img_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9546810d",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_pool = model.txt_pool \n",
    "img_pool = model.img_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "00b2dbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_loader = iter(train_loader)\n",
    "batch = next(gen_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8dff4871",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(gen_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3c6c9d",
   "metadata": {},
   "source": [
    "#### Images embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c93544d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(batch) == dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f0f9e0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch = batch['image']\n",
    "input_batch = input_batch.to(model.img_enc.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8fe76096",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tensor = img_enc(input_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cc8d86d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_batch: \t torch.Size([10, 3, 224, 224])\n",
      "img_tensor: \t torch.Size([10, 2048, 49])\n"
     ]
    }
   ],
   "source": [
    "print(\"input_batch: \\t\", input_batch.shape)\n",
    "print(\"img_tensor: \\t\", img_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3fa3de2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_embed  = model.embed_image_features(img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "085d2693",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = img_enc.cnn(input_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8abc3a19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 2048, 7, 7])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d60a42",
   "metadata": {},
   "source": [
    "#### Text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d1e2894c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "339e36e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_tensor, lengths = txt_enc(input_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cc898b06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 31, 2048])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9a9f1fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_tensor = model.embed_caption_features(txt_tensor, lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0f853ab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 31, 2048])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dca24866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13, 9, 31, 10, 6, 11, 29, 10, 7, 10]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a790821b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 31])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['caption'][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b33fa9",
   "metadata": {},
   "source": [
    "#### Forward batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dd9e091b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 224, 224])\n",
      "torch.Size([10, 31])\n"
     ]
    }
   ],
   "source": [
    "print(batch['image'].shape)\n",
    "print(batch['caption'][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d1fbb0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_embed, txt_embed = model.forward_batch(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ae68371c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_embed torch.Size([10, 2048, 49])\n",
      "txt_embed torch.Size([10, 31, 2048])\n"
     ]
    }
   ],
   "source": [
    "print(\"img_embed\", img_embed.shape)\n",
    "print(\"txt_embed\", txt_embed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b7832b",
   "metadata": {},
   "source": [
    "<font color='red'> **Deep dive on Similarity --------------------- (2)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ec9f7df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from retrieval.model.similarity import similarity as sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2c2b7033",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_obj = sim.AdaptiveEmbeddingI2T(\n",
    "    **opt.model[\"similarity\"]['params']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "701bdf14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-17 11:37:35,073 - [INFO    ] - Created similarity: AdaptiveEmbeddingI2T(\n",
      "  (norm): Normalization(\n",
      "    (norm): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  )\n",
      "  (adapt_txt): ADAPT(\n",
      "    (fc_gamma): Sequential(\n",
      "      (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    )\n",
      "    (fc_beta): Sequential(\n",
      "      (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (fovea): Fovea(smooth=10,train_smooth: False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "similarity = sim.Similarity(\n",
    "    device=img_embed.device,\n",
    "    similarity_object=sim_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6798c69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward AdaptiveI2T\n",
    "cap_embedp = txt_embed.permute(0, 2, 1)\n",
    "img_embedp = img_embed.permute(0, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "498faf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "BB, LT, KK = img_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1a95df7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LT == model.latent_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "92cf0561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cap_embedp torch.Size([10, 2048, 31])\n",
      "img_embedp torch.Size([10, 2048, 49])\n"
     ]
    }
   ],
   "source": [
    "print(\"cap_embedp\", cap_embedp.shape)\n",
    "print(\"img_embedp\", img_embedp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "168b3079",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_embedp = model.similarity.similarity.norm(cap_embedp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "df09663b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 2048, 31])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap_embedp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "82b0a482",
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = torch.zeros(\n",
    "    img_embedp.shape[0], cap_embedp.shape[0]\n",
    ").to(similarity.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c8a1987a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0af37629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global image representation\n",
    "img_embedp = img_embedp.mean(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f7409364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 2048])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_embedp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4c9797d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, img_tensor in enumerate(img_embedp):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "45011f4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2048])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "882a3e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_vector = img_tensor.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "64fe369a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2048])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "89092856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 2048, 31])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap_embedp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "809b440e",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_output = model.similarity.similarity.adapt_txt(value=cap_embedp, query=img_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "69015867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 2048, 31])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a1bfac2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_output = model.similarity.similarity.fovea(txt_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b36c2ac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 2048, 31])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e9816d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_vector = txt_output.max(dim=-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "52a39ff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 2048])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fa6a35ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from retrieval.model.similarity.measure import l2norm, cosine_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "53a024fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_vector = l2norm(txt_vector, dim=-1)\n",
    "img_vector = l2norm(img_vector, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e077b125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_vector torch.Size([10, 2048])\n",
      "img_vector torch.Size([1, 2048])\n"
     ]
    }
   ],
   "source": [
    "print(\"txt_vector\", txt_vector.shape)\n",
    "print(\"img_vector\", img_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f540aa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = cosine_sim(img_vector, txt_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6437227d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d68eb30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = sim.squeeze(-1)\n",
    "sims[i,:] = sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "930b182b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e700b8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = torch.zeros(\n",
    "            img_embedp.shape[0], cap_embedp.shape[0]\n",
    "        ).to(model.similarity.device)\n",
    "\n",
    "#Â Loop\n",
    "for i, img_tensor in enumerate(img_embedp):\n",
    "    img_vector = img_tensor.unsqueeze(0)\n",
    "    txt_output = model.similarity.similarity.adapt_txt(value=cap_embedp, query=img_vector)\n",
    "    txt_output = model.similarity.similarity.fovea(txt_output)\n",
    "    txt_vector = txt_output.max(dim=-1)[0]\n",
    "    txt_vector = l2norm(txt_vector, dim=-1)\n",
    "    img_vector = l2norm(img_vector, dim=-1)\n",
    "    sim = cosine_sim(img_vector, txt_vector)\n",
    "    sim = sim.squeeze(-1)\n",
    "    sims[i,:] = sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a133ff78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6651, 0.6212, 0.6875, 0.6416, 0.6255, 0.6668, 0.6969, 0.6564, 0.6269,\n",
       "         0.6352],\n",
       "        [0.5615, 0.5528, 0.5802, 0.5712, 0.5328, 0.5683, 0.5993, 0.5580, 0.5433,\n",
       "         0.5589],\n",
       "        [0.6139, 0.5952, 0.6394, 0.6137, 0.5862, 0.6269, 0.6562, 0.6014, 0.5851,\n",
       "         0.5999],\n",
       "        [0.6303, 0.6114, 0.6458, 0.6136, 0.5981, 0.6273, 0.6579, 0.6276, 0.5954,\n",
       "         0.6078],\n",
       "        [0.6168, 0.6007, 0.6551, 0.6016, 0.5901, 0.6332, 0.6643, 0.6190, 0.5978,\n",
       "         0.5954],\n",
       "        [0.6279, 0.6105, 0.6594, 0.6227, 0.6050, 0.6505, 0.6662, 0.6333, 0.6054,\n",
       "         0.6117],\n",
       "        [0.6615, 0.6251, 0.6819, 0.6493, 0.6370, 0.6686, 0.7018, 0.6481, 0.6418,\n",
       "         0.6342],\n",
       "        [0.6455, 0.6314, 0.6830, 0.6615, 0.6256, 0.6717, 0.6945, 0.6605, 0.6277,\n",
       "         0.6413],\n",
       "        [0.6522, 0.6242, 0.6739, 0.6367, 0.6214, 0.6589, 0.6996, 0.6566, 0.6106,\n",
       "         0.6315],\n",
       "        [0.6272, 0.5965, 0.6526, 0.6178, 0.6004, 0.6255, 0.6654, 0.6303, 0.5992,\n",
       "         0.5966]], device='cuda:0', grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205b1bc2",
   "metadata": {},
   "source": [
    "<font color='red'> **Deep dive on SIMILARITY --------------------- (2)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85b2783",
   "metadata": {},
   "source": [
    "<font color='red'> **Finish Deep dive on Retrieval --------------------- (1)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
